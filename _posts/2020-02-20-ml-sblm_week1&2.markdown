---
layout: post
title:  "Sample-based Learning Methods - Week1"
subtitle:  "SBLM_Week1"
categories: ml
tags: rl
comments: true

---

- 작성 중.

---  
# 2. Sample-based Learning Methods - Week1  
## 2.1 Course Introduction  
- 많은 sample-based learning method: value function과 Dynamic Programming의 아이디어에 기반  
  - ex) `Temporal Difference Learning`  
  
# 2. Sample-based Learning Methods - Week2  
## 2.1 Introduction to Monte Carlo Methods  

### 2.1.1 What is Monte Carlo?  
- 몬테카를로: 반복적인 랜덤 샘플링에 의존하는 estimation method를 지칭  

  ```  
  In RL, Monte Carlo methods allow us to estimate values directly from experience, 
  from sequences of states, actions and rewards. 
  ```  
  
  - `Learning from experience`  
    : 실로 놀라운 것  
    - environments dynamics에 대한 사전 지식이 없이도 value function을 정확히 estimate할 수 있기 때문.  

 
- 앞서 RL과 Dynamic Progamming을 연관지어 설명  
  - Pure DP에서는 agent가 environment's transition probabilities (`p`)에 대해 알아야만 했음.  
    - 근데 어떤 문제에서는 이 `p`를 모른다.  
      ex) 기상학자가 날씨 예측하려고 한다고 하자.  
          날씨의 변화는 다양한 환경적 요소들의 영향을 받는다.  
          미래의 계절 패턴에 대해 정확한 확률을 알 수 없다!  
    - 심지어는, 꽤나 합리적이어 보이는 task에 대해서도 이 `p`를 계산하기는 어렵다.  
      ex) 주사위 12개 굴렸을 때의 결과 예측  
        - 몬테카를로가 도움이 될 수 있다!!!`  
          - 몬테카를로는 소모적인 sweep을 하지 않음.  
          
- RL: Value function을 학습(learn)하길 원함.  
  - Value functions represent expected returns.  
    - 몬테카를로에서는  
    
      ```  
      해당 Expected Return들을 여러개(Multiple) 구해서 average 
      └→ Estimate the expected return from that state
      
      샘플의 수가 많아지면 avg는 expected return에 더욱 가까워짐.  
      ```  
      
      `The more returns the agent observes from a state, the more likely it is that the sample average is close to the state value.`  
      - 근데 이 return들은 episode의 끝에서만 관찰될 수 있음  
        → 그래서 몬테카를로 방법을 epsodic task에서만 보기로.  
        
  - 몬테카를로: MAB문제와 비슷한 것처럼 보  
    - __하지만 몬테카를로는 arm 대신 `policy`를 고려.__  
      
    ![](http://drive.google.com/uc?export=view&id=18L_sVLFqqgzgtf0AVY_8Q8FYEqXW-bhd)  
    - Introduce list of returns one for each state.  
    - Each list holds the returns observed from state S.  
    - Generate an episode by following our policy.  
    - For each date in the episode, we compute the return and stock(?) in the list of returns,  
      but how in efficient way?   
    ![](http://drive.google.com/uc?export=view&id=18ReulVyCuJwyAdgprzT0AyCVDgQ9EVj-)  
    ![](http://drive.google.com/uc?export=view&id=1Znn8jqJgRu5uPC6_zePan8-Mx_zBX8N7)   
    - `By looking backwards from the terminal time-step, we can efficiently compute the returns for each state encountered during the episode.` 
      `The first return in just the last rewards.`  
    - So we add teh last reward to the list of returns for ![](https://latex.codecogs.com/gif.latex?S_%7Bt-1%7D)  
    - Then, we set the value of ![](https://latex.codecogs.com/gif.latex?S_%7Bt-1%7D) to be the avg of return ![](https://latex.codecogs.com/gif.latex?S_%7Bt-1%7D)   
    - ...  
    
    - __Incremental Update__  
      `NewEstimate ← OldEstimate + StepSize[Target - OldEstimate]`  
      
#### Summary  

```  
- We talked about how Monte Carlo methods learn directly from interaction.  
  (Notably, they don't need a model of the environment dynamics.)  
- We showed a Monte Carlo algorithm for learning state-values in episodic problems.  
```  
      
### 2.1.2 Using Monte Carlo for Prediction  

- 블랙잭 예시  
  [블랙잭 룰](https://www.7luck.com/JSPVIEW/default?URL_JSP=--guid--GUID_04_01_02&sel_lang_typ=KR)  
  
  ```  
  - 블랙잭 설명  
    - 52개 카드  
    - 21일 넘지 않는 선에서 가진 카드의 합이 가능한 한 크도록 카드 모으기  
    - 얼굴 카드: 10  
    - 각 ace는 플레이어의 선호에 따라 1 또는 11로 계산할 수 있음  
    
    - 시작할 때 플레이어와 딜러에게 두 장의 카드 씩을 줌  
    - 플레이어는 딜러가 가진 두 장의 카드 중 한 장을 볼 수 있음. (나머지 한 장은 뒤집어 놓음)  
    - 시작부터 플레이어가 합계 21인 카드를 갖고 있으면, 플레이어 승 (단, 딜러도 합계 21이면 무승부)  
      - 21 넘지 않으면, hit 선언할 수 있음. (카드 한 장 더 받기)  
      - 21을 넘으면 플레이어 패  
      - 플레이어가 그만 받겠다고 하면, 이제 딜러 차례  
      
    - 딜러는 자신의 카드 합이 17 미만이면 hit  
      - 딜러이 합이 21 넘으면 플레이어 승  
      - 안 넘으면 둘 중에 21에 가까운 사람이 승  
  ```  
  
  - 블랙잭을 undiscounted MDP 문제로 생각해볼 수 있음. (매 경기가 하나의 episode)  
  - reward:  이기면1, 비기면0, 지면-1  
  - action(2가지): `hit` or `stick`  (hit: 카드 더 받기, stick: 카드 그만 받기)  
  - state(3가지):  
    - 플레이어에게 usable한 ace가 있는가. (Y/N)  
    - 플레이어 카드의 합계 (12-21)  
    - 딜러가 보여주는 카드 (Ace-10)  
    (이 세 가지 states에 근거해 플레이어는 action 선택)  
  - Cards are dealt from a deck with replacement  
  - Policy: Stops requesting cards when player's sum is 20 or 21  
  
  - 'Now let's look at the states starting from the end of the epoisode and working backwards.  
  ![](http://drive.google.com/uc?export=view&id=10qEvoADKb3hnAgcMV9bB_GhqT_RbCxiH)  
  ![](http://drive.google.com/uc?export=view&id=16D4dle1NP0jaddX7ytr7fXY7XDhzQOJe)  
  
  - 이제 이런 블랙잭 게임을 여러 번 했다라고 한다면?  
    
    ![](http://drive.google.com/uc?export=view&id=1gf6eWy58Xh_Cot5YqLnjlwisPwHvhRiA)  
    - 먼저 10000 episode case를 보자.  
      - 세 개의 축 각각은 'the card the dealer is showing', 'agent sum', 'the value of that state'  
      - usable ace 경우에서의 plot은 no usable의 plot보다 훨씬 울퉁불퉁  
        : 왜냐하면, usable ace가 있는 경우가 상당히 적기 때문에 샘플 수가 적어서 그런 것  
      - 두 경우 모두 plot의 모양 자체는 비슷  
        - 딜러가 보여주는 카드는 value function에 그닥 큰 영향 x  
        - agent's sum은 20,21일 때 value function이 훨씬 큼  
          : agent가 따르는 policy와 연관이 깊음 (sum이 19이하면 bust)   
          
    - 500000 episodes  
      - Estimates have nearly converged to the state values.  
      - 플롯이 훨씬 smoother  
      - 10000 case와 똑같은 패턴  
      
- 결론적으로 Monte Carlo Learning은    
  - experience로 부터 직접적으로 학습 (no need to keep a large model of the env.)   
  - 각 state에서의 value를 다른 state에서의 value들과는 독립적으로 추정할 수 있음.  
    (cf. Dynamic Programming: 어떤 state의 value는 다른 state의 value에 의존했었음)  
  - 각 state에서의 value 업데이트를 위한 연산은 MDP의 크기에 의존하지 않음.  
    - 오히려, 에피소드의 길이에 의존  
    
#### Summary  

```  
- Showed how to use Monte Carlo Prediction to learn the value function of a policy
- Talked about how Monte Carlo learning is computationally efficient
  (모든 MDP를 sweep할 필요가 없음) 
```   

cf) on-policy, off-policy  

---  

## 2.2 Monte Carlo for Control  
### 2.2.1 Using Monte Carlo for Action Values  
- 앞의 강의: 고정된 policy에 대해 sate-value fuction 학습하기.  
- 이번 강의: action value 학습하기.  
  
- action value 학습: state value 학습과 정확히 똑같은 과정  
  - state-action pair로부터 `return` 모아서 평균내기  
    ![](https://latex.codecogs.com/gif.latex?q_%7B%5Cpi%7D%28s%2Ca%29%20%5Cdoteq%20E_%7B%5Cpi%7D%5BG_t%7CS_t%3Ds%2C%20A_t%3Da%5D)  
    
- 그런데, action value 학습을 왜 신경쓰나?  
  - action value는 policy 학습에 유용  
    - 똑같은 state 하에서 다른 action들을 비교하게 해줌 (더 좋은 action으로 switch 가능)   
      ![](https://latex.codecogs.com/gif.latex?%5Cunderset%7Ba%7D%7Bargmax%7Dq_%7B%5Cpi%7D%28s%2Ca%29)  
      
- 하지만, 또 문제가 있음.  
  - policy에 의해, 단 한 번도 선택된 적 없는 action이라면, 그 action에 상응하는 return을 절대 observe하지 못할 것.  
    - 정확한 Monte Carlo Estimate를 얻을 수 없다.  
      (agent는 value를 학습하려면 매 state마다 가능한 action들을 모두 시도해야만 함.)  
      ('This is the problem of maintaining exploration in RL)  
      
      ```  
      예를 들어, 집으로 가는 새로운 길이 만들어짐. 이 길은 안 가 봄. 
      이 길을 가보지 않는다면 이 길이 기존의 길보다 나은지 아닌지 알 수 없음  
      ```  
      
- `Exploring Starts`  
  : one way to maintain exploration  
  - 에피소드들이 모든 state-action pair에서 시작한다는 것을 보장해야 함  
  - 그 후에는 agent가 그냥 policy를 따라가  
  
  ![](http://drive.google.com/uc?export=view&id=1Eq1Lka-_ilzjCV2IXDIBdrGjHGWT1JZF)  
  - Exploring Starts는 파란색으로 표시된 것처럼, 빨간색으로 표시된 policy가 향하는 방향과는 다르게, 
    에피소드의 시작으로서 랜덤하게 state, action을 샘플링  
  - 그 뒤로는 그냥 에피소드가 끝날 때까지 빨간 policy를 따라가  
  - deterministic policy를 evaluate 하기 위해, 이런 방식으로 start state를 셋팅할 수 있어야 함  
  - 이 방식이 항상 간으한 건 아님.  
    - stochastic policy를 evaluate 하기 위해서는 epsilon-greedy 같은 다른 exploration strategy등을 사용 가능  
    
#### Summary  

```  
- Discribed a Monte Carlo algorithm for estimating action-values  
- Discussed the importance of maintaining exploration  
```  
  
  
### 2.2.2 Using Monte Carlo methods for generalized policy iteration  
- 이 강의에서 배울 것  
  ```  
  - Understand how to use Monte Carlo methods to implement a Generalized Policy Iteration algorithm
  ```  
  
- GPI: policy evaluation & policy improvement  
  - policy: 계속 향상됨  
  - Policy Improvement Step:   
    ![](https://latex.codecogs.com/gif.latex?%5Cpi_%7Bk&plus;1%7D%28s%29%20%5Cdoteq%20%5Cunderset%7Ba%7D%7Bargmax%7Dq_%7B%5Cpi_k%7D%28s%2Ca%29)  
    - can make a policy greedy with respect to the agent's current action value estimates.  
  - Policy Evaluation Step:  
    : 몬테카를로 사용  (action value 추정을 위해)  
    - will use Monte Carlo method to estimate the action values.  
    
  - 수렴을 위해서는 estimate이 계속해서 improve 되는 것이 필요하지, ...

    

