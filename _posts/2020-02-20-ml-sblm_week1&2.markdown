---
layout: post
title:  "Sample-based Learning Methods - Week1"
subtitle:  "SBLM_Week1"
categories: ml
tags: rl
comments: true

---

- 작성 중.

---  
# 2. Sample-based Learning Methods - Week1  
## 2.1 Course Introduction  
- 많은 sample-based learning method: value function과 Dynamic Programming의 아이디어에 기반  
  - ex) `Temporal Difference Learning`  
  
# 2. Sample-based Learning Methods - Week2  
## 2.1 Introduction to Monte Carlo Methods  

### 2.1.1 What is Monte Carlo?  
- 몬테카를로: 반복적인 랜덤 샘플링에 의존하는 estimation method를 지칭  

  ```  
  In RL, Monte Carlo methods allow us to estimate values directly from experience, 
  from sequences of states, actions and rewards. 
  ```  
  
  - `Learning from experience`  
    : 실로 놀라운 것  
    - environments dynamics에 대한 사전 지식이 없이도 value function을 정확히 estimate할 수 있기 때문.  

 
- 앞서 RL과 Dynamic Progamming을 연관지어 설명  
  - Pure DP에서는 agent가 environment's transition probabilities (`p`)에 대해 알아야만 했음.  
    - 근데 어떤 문제에서는 이 `p`를 모른다.  
      ex) 기상학자가 날씨 예측하려고 한다고 하자.  
          날씨의 변화는 다양한 환경적 요소들의 영향을 받는다.  
          미래의 계절 패턴에 대해 정확한 확률을 알 수 없다!  
    - 심지어는, 꽤나 합리적이어 보이는 task에 대해서도 이 `p`를 계산하기는 어렵다.  
      ex) 주사위 12개 굴렸을 때의 결과 예측  
        - 몬테카를로가 도움이 될 수 있다!!!`  
          - 몬테카를로는 소모적인 sweep을 하지 않음.  
          
- RL: Value function을 학습(learn)하길 원함.  
  - Value functions represent expected returns.  
    - 몬테카를로에서는  
    
      ```  
      해당 Expected Return들을 여러개(Multiple) 구해서 average 
      └→ Estimate the expected return from that state
      
      샘플의 수가 많아지면 avg는 expected return에 더욱 가까워짐.  
      ```  
      
      `The more returns the agent observes from a state, the more likely it is that the sample average is close to the state value.`  
      - 근데 이 return들은 episode의 끝에서만 관찰될 수 있음  
        → 그래서 몬테카를로 방법을 epsodic task에서만 보기로.  
        
  - 몬테카를로: MAB문제와 비슷한 것처럼 보임  
    - __하지만 몬테카를로는 arm 대신 `policy`를 고려.__  
      
    ![](http://drive.google.com/uc?export=view&id=18L_sVLFqqgzgtf0AVY_8Q8FYEqXW-bhd)  
    - Introduce list of returns one for each state.  
    - Each list holds the returns observed from state S.  
    - Generate an episode by following our policy.  
    - For each date in the episode, we compute the return and stock(?) in the list of returns,  
      but how in efficient way?   
    ![](http://drive.google.com/uc?export=view&id=18ReulVyCuJwyAdgprzT0AyCVDgQ9EVj-)  
    ![](http://drive.google.com/uc?export=view&id=1Znn8jqJgRu5uPC6_zePan8-Mx_zBX8N7)   
    - `By looking backwards from the terminal time-step, we can efficiently compute the returns for each state encountered during the episode.` 
      `The first return in just the last rewards.`  
    - So we add teh last reward to the list of returns for ![](https://latex.codecogs.com/gif.latex?S_%7Bt-1%7D)  
    - Then, we set the value of ![](https://latex.codecogs.com/gif.latex?S_%7Bt-1%7D) to be the avg of return ![](https://latex.codecogs.com/gif.latex?S_%7Bt-1%7D)   
    - ...  
    
    - __Incremental Update__  
      `NewEstimate ← OldEstimate + StepSize[Target - OldEstimate]`  
      
      
### 2.1.2 Using Monte Carlo for Prediction  

- 블랙잭 예시  
  [블랙잭 룰](https://www.7luck.com/JSPVIEW/default?URL_JSP=--guid--GUID_04_01_02&sel_lang_typ=KR)  
  
  ```  
  - 블랙잭 설명  
    - 52개 카드  
    - 21일 넘지 않는 선에서 가진 카드의 합이 가능한 한 크도록 카드 모으기  
    - 얼굴 카드: 10  
    - 각 ace는 플레이어의 선호에 따라 1 또는 11로 계산할 수 있음  
    
    - 시작할 때 플레이어와 딜러에게 두 장의 카드 씩을 줌  
    - 플레이어는 딜러가 가진 두 장의 카드 중 한 장을 볼 수 있음. (나머지 한 장은 뒤집어 놓음)  
    - 시작부터 플레이어가 합계 21인 카드를 갖고 있으면, 플레이어 승 (단, 딜러도 합계 21이면 무승부)  
      - 21 넘지 않으면, hit 선언할 수 있음. (카드 한 장 더 받기)  
      - 21을 넘으면 플레이어 패  
      - 플레이어가 그만 받겠다고 하면, 이제 딜러 차례  
      
    - 딜러는 자신의 카드 합이 17 미만이면 hit  
      - 딜러이 합이 21 넘으면 플레이어 승  
      - 안 넘으면 둘 중에 21에 가까운 사람이 승  
  ```  
  
  - 블랙잭을 undiscounted MDP 문제로 생각해볼 수 있음. (매 경기가 하나의 episode)  
  - reward:  이기면1, 비기면0, 지면-1  
  - action(2가지): `hit` or `stick`  (hit: 카드 더 받기, stick: 카드 그만 받기)  
  - state(3가지):  
    - 플레이어에게 usable한 ace가 있는가. (Y/N)  
    - 플레이어 카드의 합계 (12-21)  
    - 딜러가 보여주는 카드 (Ace-10)  
    (이 세 가지 states에 근거해 플레이어는 action 선택)  
  - Cards are dealt from a deck with replacement  
  - Policy: Stops requesting cards when player's sum is 20 or 21  
  
  - 'Now let's look at the states starting from the end of the epoisode and working backwards.  
  ![](http://drive.google.com/uc?export=view&id=10qEvoADKb3hnAgcMV9bB_GhqT_RbCxiH)  
  ![](http://drive.google.com/uc?export=view&id=16D4dle1NP0jaddX7ytr7fXY7XDhzQOJe)  
  
  - 이제 이런 블랙잭 게임을 여러 번 했다라고 한다면?  
    
    ![](http://drive.google.com/uc?export=view&id=1gf6eWy58Xh_Cot5YqLnjlwisPwHvhRiA)  
    - 먼저 10000 episode case를 보자.  
      - 세 개의 축 각각은 'the card the dealer is showing', 'agent sum', 'the value of that state'  
      - usable ace 경우에서의 plot은 no usable의 plot보다 훨씬 울퉁불퉁  
        : 왜냐하면, usable ace가 있는 경우가 상당히 적기 때문에 샘플 수가 적어서 그런 것  
      - 두 경우 모두 plot의 모양 자체는 비슷  
        - 딜러가 보여주는 카드는 value function에 그닥 큰 영향 x  
        - agent's sum은 20,21일 때 value function이 훨씬 큼  
          : agent가 따르는 policy와 연관이 깊음 (sum이 19이하면 bust)   
          
    - 500000 episodes  
      - Estimates have nearly converged to the state values.  
      - 플롯이 훨씬 smoother  
      - 10000 case와 똑같은 패턴  
      
- 결론적으로 Monte Carlo Learning은    
  - experience로 부터 직접적으로 학습 (no need to keep a large model of the env.)   
  - 각 state에서의 value를 다른 state에서의 value들과는 독립적으로 추정할 수 있음.  
    (cf. Dynamic Programming: 어떤 state의 value는 다른 state의 value에 의존했었음)  
  - 각 state에서의 value 업데이트를 위한 연산은 MDP의 크기에 의존하지 않음.  
    - 오히려, 에피소드의 길이에 의존  
    
cf) on-policy, off-policy  

---  

## 2.2 Monte Carlo for Control  
### 2.2.1 Using Monte Carlo for Action Values  
- 앞의 강의: 고정된 policy에 대해 state-value fuction 학습하기.  
- 이번 강의: action value 학습하기.  
  
- action value 학습: state value 학습과 정확히 똑같은 과정  
  - state-action pair로부터 `return` 모아서 평균내기  
    ![](https://latex.codecogs.com/gif.latex?q_%7B%5Cpi%7D%28s%2Ca%29%20%5Cdoteq%20E_%7B%5Cpi%7D%5BG_t%7CS_t%3Ds%2C%20A_t%3Da%5D)  
    
- 그런데, action value 학습을 왜 신경쓰나?  
  - action value는 policy 학습에 유용  
    - 똑같은 state 하에서 다른 action들을 비교하게 해줌 (더 좋은 action으로 switch 가능)   
      ![](https://latex.codecogs.com/gif.latex?%5Cunderset%7Ba%7D%7Bargmax%7Dq_%7B%5Cpi%7D%28s%2Ca%29)  
      
- 하지만, 또 문제가 있음.  
  - policy에 의해, 단 한 번도 선택된 적 없는 action이라면, 그 action에 상응하는 return을 절대 observe하지 못할 것.  
    - 정확한 Monte Carlo Estimate를 얻을 수 없다.  
      (agent는 value를 학습하려면 매 state마다 가능한 action들을 모두 시도해야만 함.)  
      ('This is the problem of maintaining exploration in RL)  
      
      ```  
      예를 들어, 집으로 가는 새로운 길이 만들어짐. 이 길은 안 가 봄. 
      이 길을 가보지 않는다면 이 길이 기존의 길보다 나은지 아닌지 알 수 없음  
      ```  
      
- `Exploring Starts`  
  : one way to maintain exploration  
  - 에피소드들이 모든 state-action pair에서 시작한다는 것을 보장해야 함  
  - 그 후에는 agent가 그냥 policy를 따라가  
  
  ![](http://drive.google.com/uc?export=view&id=1Eq1Lka-_ilzjCV2IXDIBdrGjHGWT1JZF)  
  - Exploring Starts는 파란색으로 표시된 것처럼, 빨간색으로 표시된 policy가 향하는 방향과는 다르게, 
    에피소드의 시작으로서 랜덤하게 state, action을 샘플링  
  - 그 뒤로는 그냥 에피소드가 끝날 때까지 빨간 policy를 따라가  
  - deterministic policy를 evaluate 하기 위해, 이런 방식으로 start state를 셋팅할 수 있어야 함  
  - 이 방식이 항상 간으한 건 아님.  
    - stochastic policy를 evaluate 하기 위해서는 epsilon-greedy 같은 다른 exploration strategy등을 사용 가능  
  
  
### 2.2.2 Using Monte Carlo methods for generalized policy iteration  
  
- GPI: policy evaluation & policy improvement  
  - policy: 계속 향상됨  
  - Policy Improvement Step:   
    ![](https://latex.codecogs.com/gif.latex?%5Cpi_%7Bk&plus;1%7D%28s%29%20%5Cdoteq%20%5Cunderset%7Ba%7D%7Bargmax%7Dq_%7B%5Cpi_k%7D%28s%2Ca%29)  
    - can make a policy greedy with respect to the agent's current action value estimates.  
  - Policy Evaluation Step:  
    : 몬테카를로 사용  (action value 추정을 위해)  
    - will use Monte Carlo method to estimate the action values.  

- action value 학습 Monte Carlo method   
  ![](http://drive.google.com/uc?export=view&id=1UakU0qfUaFf8Ep4iuiFGUJU7HtjN_v_s)  
  - 이 과정을 episode 동안의 관찰되는 모든 state에서 행함  
  - dynamic programming보다 훨씬 쉽네.  
  
  - 이것이 `generalized policy iteration algorithm , Monte Carlo with exploring starts`  


### 2.2.3 Solving the Blackjack Example  

- agent로 하여금 Exploring Starts 몬테카를로를 이용해 블랙잭을 플레이 하도록 학습시킬 것  

- Exploring Starts  
  : 에피소드가 random state와 random action에서 시작할 것을 요구  
  
  - 블랙잭의 경우, 다행히도, random state에서 시작함.  
  - 이제, 각 epsiode마다 첫 action을 random하게 선택하기만 하면 됨  
    = agent가 첫 state에서 꼭 best action이라고 생각하는 action을 뽑는 게 아니라는 말.  
      (이 경우 hit이나 stick 중 아무 action이나 선택)  
      
    ```  
    첫 policy: 
    - agent sum < 20: hit
    - agent sum == 20 or 21: stick  
    ```  
    
    ```  
    # ex) 
    상황: agent 카드의 총합: 13, ace는 없음, 딜러의 visible 카드는 8 (invisible 카드는 5라고 가정) 
    - randomm action에 따라 agent가 'hit'을 한다고 가정.  
    
    - 이윽고, agent가 7을 받았고 합은 20 
      - agnet는 policy에 따라 stick하기로 결정 
     
    - 이제 딜러의 차례  
      - 딜러가 9를 받으면, 21을 넘기에, agent 승! (agent의 reward +1) 
    ```  
   
    - 해당 사례에서 등장한 state-action pair? (에피소드의 끝에서부터 역순으로 올라가겠음)  
   
      - `X=(NoAce, 20, 8)`  
      - `Y=(NoAce, 13, 8)`  
    
      |S,A|Returns(S,A)|Q(S,A)|π(S)|  
      |---|------------|------|----|  
      |X,Stick|Returns(X,Stick)=[1]|Hit:0 , Stick:1|Stick|  
      |Y,Hit|Returns(Y,Hit)=[1]|Hit:1 , Stick:0|Hit|  
      
      - 이런 process를 많은 episode들에 대해 반복  
        - Action value와 Policy가 각각의 Optimal Value로 다가갈 것  

   
  - 오랫동안 시행해보고 난 뒤에 agent가 찾은 optimal policy  
  ![](http://drive.google.com/uc?export=view&id=1R0ZyobxgTBZiZHS6Aq1b1cuzFYIITRoH)  
    - agent에게 ace가 있을 때  
      : Policy가 훨씬 aggressive. (카드의 합 계산에 있어 flexibility가 더 크기 때문)  
    - agent에게 ace가 없을 때  
      : 딜러가 보여주는 카드에 훨씬 더 크게 의존  
      - 예를 들어, 딜러가 보여주는 카드가 2, 3 정도로 낮을 경우 agent가 가진 카드의 sum이 13이상이면, stick을 하는데, 이러면 이길 확률이 낮은 것처럼 보여도  optimal임.  
      

---  
## 2.3 Exploration Methods for Monte Carlo  
### 2.3.1 Epsilon-soft policies  

- Exploring Starts를 사용할 수 없는 경우?  
  - 이 알고리즘은, Every possible State-action pair에서 출발할 수 있어야만 함.  
    - 그렇지 않다면, agent가 충분히 explore하지 못하고 sub-optimal에 빠질 가능성 농후  
    
  - Q: 자율주행차: 어떻게 agent가 모든 가능 state에서 출발할 수 있다고 보장할 수 있는가?  
    - 이를 위해서는 자동차를 바쁜 도로 상 다양한 곳에 차를 놓아야할텐데, 이는 너무 위험하고 비현실적인 얘기.  
    
  - 그렇다면, exploring start 없이 모든 action value를 학습할 수 있는 방법은 없을까?  
    : Epsilon-greedy Exploration을 떠올려 보라!  (⊂ ε-Soft Policies)  
  
- ε-Soft Policies  
  - Epsilon-greedy Exploration ⊂ `ε-Soft Policies`  
  - 각 action을 `ε / # of actions`의 확률로 취함  
  - agent로 하여금 계속해서 explore하도록 만듦.  
    - 이 말은, Monte Carlo Control 알고리즘에서 exploring starts 조건을 drop할 수 있다는 것.  
    - 모든 state-action pair를 무기한으로 방문할 수 있도록 하기 때문에 모든 state의 모든 action에 0이 아닌 확률값을 부여!  

  - Stochastic Policy! (Not Deterministic)  

  - Episode들이 쌓이면 모든 sate에서 결국에는 모든 action을 한 번쯤은 취할 것  
  
    ![](https://drive.google.com/open?id=1q2oEfEyXStiPRKBysiVH-vluchRqj3Kl)  
   
- ε-Soft policies may not be optimal  
  ![](http://drive.google.com/uc?export=view&id=1ra2_Wo9s-CCI1dwn-DQH0BrZtqcrEMcu)  
  - Deterministic optimal policy로 수렴하는 것은 불가.   
  - ε-Soft policy는 optimal ε-Soft policy를 찾는 데에만 사용.  
  
  - ε-Soft policies: 일반적으로는 optimal policy보다 나쁜 결과.  
  - 하지만 때때로는 상당히 잘 수행하기 때문에 exploring starts를 사용하지 않을 수 있다!  
  
- pseudo-code  
  ![](https://drive.google.com/open?id=1RW8K6unzQNmXnv18XX8uqMcay1KTeHIN)  
  - Exploring Starts와의 차이점을 보면 이해하기 쉬울 것  
    - one change to initial conditions  
    - one change to policy evaluation  
    - one change to the policy improvement step  
    
  - policy improvement 단계에서는 ε-greedy policy가 사용됐는데, 'Optimal ε soft policy is an ε greedy policy'라는 점을 이해하면 이해가 될 것  

---  
## 2.4 Off-policy Learning for Prediction  
### 2.4.1 Why does off-policy learning matter?  

- π-soft policy  
  - continual exploration problem을 해결!  
  - 하지만, reward나 action의 모두에서 `suboptimal`일 뿐  
  
- On-Policy & Off-Policy  
  - On-Policy  
    - improve and evaluate the policy being used to select actions  
    - agent는 data를 생성하는 데에 사용된 policy 학습  
  - Off-Policy  
    - improve and evaluate a different policy from the one used to select actions   
    - agent는 각기 다른 policy를 따라 생성된 data로부터 policy에 대해 학습  
    
- agent가 학습 중인 policy를 `target policy`라 부름. (agent의 target이기 때문에)  
  - Target Policy: 보통 π로 표시  
  - Target Policy를 위해 value function을 학습  
  - ex) optimal policy  
  
- action을 선택하는 데에 agent가 사용하는 policy를 'behavior policy'라 부름 (agent의 behavior을 규정짓기 때문에)  
  - Behavior Policy: 보통 b로 표시  
  - Behavior Policy로부터 action을 선택  
  - 일반적으로 exploratory policy  
  
- Target Policy vs Behavior Policy  
  - Target Policy에 따라서만 행동하면, 지극히 적은 수의 state만을 경험할 것  
  - Exploration을 좋아하는 policy(Behavior Policy)를 따라 behave할 수 있다면 훨씬 많은 수의 state을 경험할 수 있음  

  ![](http://drive.google.com/uc?export=view&id=1DHamGi2QXs6yzP7M6axc9or_wXpu2dP0)  
  
  
- off-policy learning 의 적용  
  - ex) Learning from demonstration, parallel learning  
  
- off-policy의 key-rule  
  - Behavior policy가 Target policy를 커버해야 한다는 점   
  - `π(a|s)>0 where b(a|s)>0`  
  
  - ex)  
    ![](http://drive.google.com/uc?export=view&id=1ZPlV5xY0h6xMxYsZLrZzrXCMBTJxHKoA)  
    - 이 경우,behavior policy는 항상 위로 가는데, target policy는 오른쪽으로 가.  
    - 이 때, agent는 오른쪽으로 갈 때, 가령 100000 reward를 얻는다고 하더라도, 이를 알지 못할 것  
  
  - on-policy learning: off-policy learning의 generalization  
    - on-policy learning: `π(a|s)>0 = b(a|s)>0`  

### 2.4.2 Importance Sampling  

- Importance Sampling이 해결할 수 있는 문제  
  - b라는 분포로부터 샘플링 된 확률변수 X  
  - 타겟분포 π의 관점에서 확률변수 X의 기대값을 추정하려고 함  (b ≠ π)  
  
  - 결국 구하고자하는 것은  
    ![](https://latex.codecogs.com/gif.latex?E_%7B%5Cpi%7D%5BX%5D)  
    
    ![](https://latex.codecogs.com/gif.latex?E_%7B%5Cpi%7D%5BX%5D%20%5Cdoteq%20%5Csum_%7Bx%5Cin%20X%7Dx%5Cpi%28x%29)  
    ![](https://latex.codecogs.com/gif.latex?%3D%20%5Csum_%7Bx%5Cin%20X%7Dx%5Cpi%28x%29%5Cfrac%7Bb%28x%29%7D%7Bb%28x%29%7D)  
    ![](https://latex.codecogs.com/gif.latex?%3D%20%5Csum_%7Bx%5Cin%20X%7Dx%5Cfrac%7B%5Cpi%28x%29%7D%7Bb%28x%29%7Db%28x%29)  
    
    ![](https://latex.codecogs.com/gif.latex?%5Cfrac%7B%5Cpi%28x%29%7D%7Bb%28x%29%7D)  
    를 Importance Sampling Ratio라고 하고, 우리에게 중요한 것  
    
    ![](https://latex.codecogs.com/gif.latex?%3D%5Csum_%7Bx%5Cin%20X%7Dx%5Crho%28x%29b%28x%29)  
    ![](https://latex.codecogs.com/gif.latex?%3DE_b%5BX%5Crho%28X%29%5D)  
    ![](https://latex.codecogs.com/gif.latex?%5Capprox%20%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5Enx_i%5Crho%28x_i%29)  
    ![](https://latex.codecogs.com/gif.latex?x_i%20%5Csim%20b)  
    
    결론적으로  
    ![](https://latex.codecogs.com/gif.latex?E_%7B%5Cpi%7D%5BX%5D%20%5Capprox%20%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5Enx_i%5Crho%28x_i%29%2C%20%5C%3Bx_i%20%5Csim%20b)  

### 2.4.3 Off-Policy Monte Carlo Prediction  

- Recall  
  - Monte Carlo Estimation의 목적  
    - Return들의 평균을 계산해서, 각 sate의 value를 추정하는 것  
      ![](http://drive.google.com/uc?export=view&id=1IF3cMyNEqckjKqMDNOn4ppvclwFacjTV)  
      
- 그렇다면, behavior policy `b`를 따르는 return을 바탕으로 target policy `π`의 value를 추정할 수는 없을까?  
  ![](http://drive.google.com/uc?export=view&id=1WE_-05eIWYKHrzA8ZTmS_277hJ2JXTqO)  
  - 여기에서 각 sampled return들마다 ρ를 알아내면 됨!  
    
    ![](https://latex.codecogs.com/gif.latex?%5Crho%20%3D%20%5Cfrac%7BP%28trajectory%20%5C%3B%20under%5C%3B%20%5Cpi%29%7D%7BP%28trajectory%20%5C%3B%20under%5C%3B%20b%29%7D)  
    ![](https://latex.codecogs.com/gif.latex?V_%7B%5Cpi%7D%28s%29%20%3D%20E_%7Bb%7D%5B%5Crho%20G_t%7CS_t%3Ds%5D)  
    
     
- Trajectory의 확률분포  
  ![](https://latex.codecogs.com/gif.latex?P%28A_t%2C%20S_%7Bt&plus;1%7D%2C%20A_%7Bt&plus;1%7D%2C%20...%2C%20S_T%7CS_t%2C%20A_%7Bt%3AT%7D%29)  
  : agent가 state t에 있을 때, t라는 action을 취하고, 이후 시간 T가 되어 Termination이 될 때까지 state t+1에 있고, t+1 action을 취하는 일련의 과정의 확률?  
  - 모든 action들: `b`에 따라 샘플링  
  - Markov Property에 따라 이 확률분포들을 작은 chunk들로 쪼갤 수 있다.  
  
    ![](https://latex.codecogs.com/gif.latex?P%28A_t%2C%20S_%7Bt&plus;1%7D%2C%20A_%7Bt&plus;1%7D%2C%20...%2C%20S_T%7CS_t%2C%20A_%7Bt%3AT%7D%29%20%3D%20b%28A_t%7CS_t%29p%28S_%7Bt&plus;1%7D%7CS_t%2CA_t%29b%28A_%7Bt&plus;1%7D%7CS_%7Bt&plus;1%7D%29p%28S_%7Bt&plus;2%7D%7CS_%7Bt&plus;1%7D%2C%20A_%7Bt&plus;1%7D%29...p%28S_t%7CS_%7Bt-1%7D%2C%20A_%7Bt-1%7D%29)  
    
    - 첫번째 chunk  
      ![](https://latex.codecogs.com/gif.latex?b%28A_t%7CS_t%29p%28S_%7Bt&plus;1%7D%7CS_t%2CA_t%29)  
    - 두번째 chunk  
      ![](https://latex.codecogs.com/gif.latex?b%28A_%7Bt&plus;1%7D%7CS_%7Bt&plus;1%7D%29p%28S_%7Bt&plus;2%7D%7CS_%7Bt&plus;1%7D%2C%20A_%7Bt&plus;1%7D%29)  
    
    - 이를 정리하면, 다음과 같은 product notation으로 나타낼 수 있음.  
      ![](https://latex.codecogs.com/gif.latex?%5Cprod_%7Bk%3Dt%7D%5E%7BT-1%7Db%28A_k%7CS_k%29p%28S_%7Bk&plus;1%7D%7CS_k%2CA_k%29)  
    
    
- 정리해보자면,  
  ![](https://latex.codecogs.com/gif.latex?P%28trajectory%5C%3Bunder%5C%3Bb%29%5C%3B%3D%5Cprod_%7Bk%3Dt%7D%5E%7BT-1%7Db%28A_k%7CS_k%29p%28S_%7Bk&plus;1%7D%7CS_k%2CA_k%29)  
  
  - Recall from above  
    ![](https://latex.codecogs.com/gif.latex?%5Crho_%7Bt%3AT-1%7D%20%5Cdoteq%20%5Cfrac%7BP%28trajectory%5C%3Bunder%5C%3B%5Cpi%29%7D%7BP%28trajectory%5C%3Bunder%5C%3Bb%29%7D)  
    ![](https://latex.codecogs.com/gif.latex?%5Cdoteq%20%5Cprod_%7BK%3Dt%7D%5E%7BT-1%7D%5Cfrac%7B%5Cpi%28A_k%7CS_k%29p%28S_%7Bk&plus;1%7D%7CS_k%2CA_k%29%7D%7Bb%28A_k%7CS_k%29p%28S_%7Bk&plus;1%7D%7CS_k%2CA_k%29%7D)  
    ![](https://latex.codecogs.com/gif.latex?%3D%20%5Cprod_%7BK%3Dt%7D%5E%7BT-1%7D%5Cfrac%7B%5Cpi%28A_k%7CS_k%29%7D%7Bb%28A_k%7CS_k%29%7D)  
    
- v_π off-policy estimating problem으로 다시 돌아와서,  
  - agent는 behavior policy b를 따라서 많은 return들을 생산.  
  - 이 각각의 return들을 ρ의 관점으로 바꿈으로써 v_π을 추정할 수 있음.  
  
    - Process  
      ![](http://drive.google.com/uc?export=view&id=1UUdW0hgsP6SxJ41G1M7uVmsRAjn9tClx)  
      
  - ρ를 incremental하게 계산하기.  
    ![](http://drive.google.com/uc?export=view&id=1kjlhY0N5sifHR7gKw4rm1BTE2KjGkWpe)  
    
    이로부터 다음과 같은 관계를 도출해볼 수 있음.  
    ![](https://latex.codecogs.com/gif.latex?W_%7Bt&plus;1%7D%20%5Cleftarrow%20W_t%5Crho_t)  

### 2.4.4 Summary  
- Monte Carlo  
  - sample-based methods  
  - 모델 사용이 불가능할 때, 혹은 모델을 수식으로 쓰기가 어려울 때 사용  
  - return들을 평균냄으로써 value function 추정  
    - 단, value update를 위해서는 full-return을 기다려야 함  
    - 이 때문에, Monte Carlo를 episodic MDP에만 사용  
  
- Monte Carlo usage in GPI  
  - Monte Carlo는 DP처럼 state-action space를 sweep하지 않음.  
    - 그래서, 모든 state-action pair에 대해 학습할 수 있는 exploration 매커니즘 필요  
  
  - Mechanism 1: Monte Carlo with `exploring starts`  
    - 매 episode의 시작마다 첫 state와 action이 random하게 선택됨  
    - 하지만 항상 feasible 하다거나 안전한 방법은 아님  
    - 때문에, 다른 두 매커니즘 소개(Mechanism2, 3)  
  - Mechanism 2: `On-policy with ε-soft policy`  
    - agent가 stochastic policy를 따르거나 학습  
    - 보통 greedy action을 취함  
    - optimal policy 대신 `near` optimal policy로 다가감  
      - exploration을 유지하면서도 near-optimal policy가 아닌 optimal policy를 찾고 싶다는 생각에 Mechanism 3 소개  
  - Mechanism 3: `Off-policy`  
    - target policy와 behavior policy에 대한 소개  
      - behavior policy: agent가 action 선택을 위해 사용하는 policy  
        - 적절한 exploratory behavior policy를 통해 agent는 `deterministic` `target` policy를 학습할 수 있음.  
      - target policy: agent가 value function으로부터 학습하는 policy  
      
    - `Importance Sampling`  
      : behavior policy로부터 추출된 샘플로부터 target policy의 expected return을 추정하는 방법  
      - Ratio가 sample을 re-weight함.  

