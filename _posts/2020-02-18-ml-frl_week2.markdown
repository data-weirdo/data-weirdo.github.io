---
layout: post
title:  "Fundamentals of Reinforcement Learning Week2"
subtitle:  "FRL_Week2"
categories: ml
tags: rl
comments: true

---

- Marcov Decision Process (MDP) 문제와 강화학습에서 사용되는 개념들을 다룬 글입니다.  

---  
## 2.1 Introduction to Markov Decision Process  
### 2.1.1 Markov Decision Processes  
- Week1의 k-Armed Bandit pb: 현실의 다양한 문제들을 담아내지는 못함  

- k-Armed bandit문제와 MDP 문제는 다르다  
    
  토끼가 먹이를 찾아 여기저기 돌아다니고 있는 상황  
  왼쪽엔 브로콜리, 오른쪽엔 당근, reward 각각 +3, +10  
  
  이번엔 왼쪽에 당근, 오른쪽에 브로콜리가 있는 상황  
  당연히 왼쪽으로 갈 것  
  
  ```  
  - k-Armed Bandit문제: `다른 상황은 다른 액션을 낳는다`는 사실을 설명하지 못함  
  ```  
  
  이번에는 왼쪽에 브로콜리, 오른쪽에 당근이 있지만 당근 오른쪽에 호랑이가 있는 상황  
  토끼가 오른쪽으로 가는 상황은 다음 상황에 영향을 미침(IMPACT) (호랑이를 마주하게 될 것)  
  
  → 당연히 action의 long-term impact를 고려하면 브로콜리를 먹으러 왼쪽으로 가야하는 게 맞을 것  
    하지만 k-Armed Bandit은 `immediate reward`에만 집중하기 때문에 당근을 먹으러 갈 것  

  - 토끼의 `Action`에 따라 `State`(Situation)이 바뀐다.  
    - Action: 토끼가 당근을 먹으러 오른쪽으로 이동  
    - Reward: +10  
    → 이제 토끼는 호랑이와 대면하게 된 State에 놓이게 됨  
    - Action: 토끼가 호랑이에게 잡아먹힌다.  
    - Reward: -100  
    OR  
    - Action: 토끼가 브로콜리를 먹으러 왼쪽으로 이동  
    - Reward: +3  
    
    `The sequence that happens depends on the actions that the rabbit takes.`  
        
- __Generalization__  
  - The agent and environment `interact` at discrete time steps. (in the MDP framework)  
  - The agent environment interaction generates a `trajectory` of experience consisting of states, actions, and rewards.  
    ![Trajectory](https://latex.codecogs.com/gif.latex?S_0%2C%20A_0%2C%20R_1%2C%20S_1%2C%20A_1%2C%20R_2%2C%20S_2%2C%20A_2%2C%20R_3%2C%20...)    
    
    (`agent` : learner / decision maker)  
    (`environment` : everything outside the agent)  
    (`reward`: special numerical values that the agent seeks to maximize over time through its choice of actions)  
  
- 그렇다면 이 interaction의 dynamics를 나타내는 방법?  
  - (bandit 문제와 마찬가지로) `확률` 개념 사용  
    ![](https://latex.codecogs.com/gif.latex?P%28s%5E%7B%27%7D%2C%20r%7Cs%2C%20a%29)  
    ![](https://latex.codecogs.com/gif.latex?%5Cdoteq%20Pr%5C%7BS_t%3Ds%27%2C%20R_t%3Dr%20%7C%20S_%7Bt-1%7D%3Ds%2C%20A_%7Bt-1%7D%3Da%5C%7D)  
    (Given a state S and action a, p tells us the joint `probability` of next state S prime and reward are.)  
  - The funciton p defines the `dynamics` of the MDP.  
    물론, (이 강의에서는) states, actions, rewards 집합이 finite하다고 가정할 것  
    - 미래의 state와 reward는 현재의 state, action에만 의존(depend)한다.  
      → `Markov property`    
      
#### 요약  
- MDPs provide a general framework for sequential decision_making.  
- The dynamics of an MDP are defined by a probability distribution.  
    
    
### 2.1.2 Examples of MDPs  
- Week1의 맨~처음의 로봇 예  

  ```  
  S = {low, high} # 밧데리 상태 
  
  A(low) = {search, wait, recharge}
  A(high) = {search, wait} # 배터리 많은데 충전하는 건 무의미한 일    
  ```  
  
  ![](http://drive.google.com/uc?export=view&id=1Am3z4cOzifXq4qwqMJoMmRntHq3gZ-oT)  
  ![](http://drive.google.com/uc?export=view&id=1yaF_Rgcb4PwkQCGBz5QvYH5HkEJ6RhRH)  

- MDP 형식은 다양한 곳에서 다양한 형태로 사용 됨  

#### 요약  
- The MDP framework can be used to formalize a wide variety of sequential decision-making problems  

---  

## 2.2 Goal of Reinforcement Learning  

### 2.2.1 The Goal of Reinforcement Learning  
- 강화학습 목적:  미래의 total(cumulative) `reward` 최대화  

- Short-term에서 좋게 보였던 것이 long-term의 관점에서는 최선의 선택이 아닐 수 있다.  
  ex) 토끼-당근-호랑이  
  
- Maximizing `total` future reward???  
  - `return`  
    ![](https://latex.codecogs.com/gif.latex?G_t%20%5Cdoteq%20R_%7Bt&plus;1%7D%20&plus;%20R_%7Bt&plus;2%7D%20&plus;%20R_%7Bt&plus;3%7D%20&plus;%20%5C%3B...)  
    여기서 ![](https://latex.codecogs.com/gif.latex?G_t): Random Variable  
    
  - maximize the expected return  
    ![](https://latex.codecogs.com/gif.latex?E%5BG_t%5D%20%3D%20E%5BR_%7Bt&plus;1%7D&plus;R_%7Bt&plus;2%7D&plus;R_%7Bt&plus;3%7D&plus;%20%5C%3B%20...%5D)  
    
    여기서, final time step T가 있다고 가정해보자. 
    (T: final time step & Random Variable)  
    
    Episodic Tasks  
    ![](https://latex.codecogs.com/gif.latex?E%28G_t%29%20%3D%20R_%7Bt&plus;1%7D%20&plus;%20R_%7Bt&plus;2%7D%20&plus;%20R_%7Bt&plus;3%7D%20&plus;%20...%20&plus;%20R_T)  
    
  - Interaction (between agent&environment)이 끝나면,  
    → `episode`라고 하는 각 chunk들로 쪼개짐  
    - 각 episode는 이전 episode들이 어떻게 끝났는지와는 상관없이(independently) 시작  
    - 각 episode들은 `terminal state`라고 불리는 final state를 갖고 있음  
      → `Episodic tasks`  
    - 끝나고나면, agent는 처음 state로 돌아옴   
    
      ```  
      ex) 체스: 체스 게임 한 판 한 판이 'episode'  
      체스 게임은 항상 reset된 상태에서 시작  
      체스 게임의 끝은 항상 checkate, draw, resignation 로 끝남  
      ```  
      
#### Summary  
- The goal of an agent is to maximize the expected return  
- In episodic tasks, the agent-environmental interaction breaks up into episodes  


### 2.2.2 Is the reward hypothesis sufficient?  
- Reward hypothesis  

  ```  
  The all of what we mean by goals and purposes can be well though of as 
  the maximization of the expected value of the cumulative sum of a received
  scalar signal (called reward).  
  ```  
  
  - 이게 처음에는 좀 안 좋게 보일 수 있어도, 실제로 이건 굉장히 
    `flexible & widely applicable`하다고 증명됨`  
    
  - 단, 최종 reward의 최대화여야지, sub-goal을 위한 reward 최대화여서는 안됨.  
    (If achieving sorts of subgoals were rewarded, then the agent might find a way to achieve  
     them without achieving the real goal.)  
   
### 2.2.3 Michael Littman: The Reward Hypothesis  
- Reward hypothesis를 믿는다면 고심하게 될 할 두 가지가 있을 것  
  1. Figuring out what rewards agent should optimize?  
  2. Design algorithms to maximize it  
  
  ⅱ. 에 대해 논의!  
  - Reward란 것을 어떻게 정의할 수 있을까?  
    - 주식시장이면 '돈'  
    - 만약, RL agent더러 온도계를 조절하라고 한다면? 흠..

    - Goal- Reward representation  
      - 1 for goal, 0 otherwise  
      - no urgency problem  
    - Action penalty representation  
      - -1 or not goal, 0 once goal reached  
      - serious problem if there's even small prob. of getting stuck and never reaching the goal.  
      
      
- "Reward Hypothesis: Very powerful & very useful for designing state-of-art agents. 
  It's a great working hypothesis that help lead us to some excellent results."  
  "But, caution not to take it too literally, we should be open to rejecting the
  hypothesis when it is oultived its usefullness."  
    - 사고1) 가령 reward maximizing이 아닌 경우들  
      - ex. risk-averse behavior (ex. minimize the chance of a worst case outcome)  
      - ex. 항상 최고의 선택을 하는 게 아니라, 여러 일들을 balance있게 하려고 할 때? (`diversity`)   
          (음악 추천 시스템)  
    - 사고2) pursuing existing reward: 과연 high-level human behavior이랑 맞는가?  
      - Blind reward pursures aren't good people.  
      - We create our 'purpose' over years, lifttimes. (philosophical)  
- `However, should entertain the possibility that maximizing rewards might just be 
  an excellent approximation of what motivates intelligent agents.`  
  
---  

## 2.3 Continuing Tasks  

### 2.3.1 Continuing Tasks  
- 앞: about episodic problems  
- But, 끝없이 agent-environment interaction이 계속 되는 경우도 존재.  
  
- Episodic Tasks와 Continuing Tasks의 차이점  

  |Episodic Tasks|Continuing Tasks|  
  |--------------|----------------|  
  |Interaction breaks naturally into episodes|Interaction goes on continually|  
  |Each episode ends in a terminal state|No terminal state|  
  |Episodes are independent|-|  
    
  - ex) 빌딩의 온도를 조절하는 스마트 온도계  
    → continuing tasks  
  
- 엥? 그럼 Future rewards를 다 더하면 Inifinite한데?  
  ![](https://latex.codecogs.com/gif.latex?G_t%20%5Cdoteq%20R_%7Bt&plus;1%7D&plus;R_%7Bt&plus;2%7D&plus;R_%7Bt&plus;3%7D&plus;%20%5C%3B%20&plus;%20R_%7Bt&plus;k%7D%20&plus;%20%5C%3B...%20%3D%20%5Cinfty%20%3F)  
  
  → Solution: `Discounting`  
  ![](https://latex.codecogs.com/gif.latex?G_t%20%5Cdoteq%20R_%7Bt&plus;1%7D&plus;%5Cgamma%20R_%7Bt&plus;2%7D&plus;%5Cgamma%5E2%20R_%7Bt&plus;3%7D&plus;%20%5C%3B%20&plus;%20%5Cgamma%5E%7Bk-1%7DR_%7Bt&plus;k%7D%20&plus;%20%5C%3B...%20%2C%200%5Cleq%20%5Cgamma%20%3C1)  
  ![](https://latex.codecogs.com/gif.latex?%3D%20%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Ek%20R_%7Bt&plus;k&plus;1%7D)  
  - 이렇게 하면 finite!  
    ![](https://latex.codecogs.com/gif.latex?G_t%20%3D%20%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7Dr%5Ek%20R_%7Bt&plus;k&plus;1%7D%20%5Cleq%20%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%20r%5Ek%20R_%7Bmax%7D%20%3D%20R_%7Bmax%7D%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7Dr%5Ek%20%3D%20R_%7Bmax%7D*%5Cfrac%7B1%7D%7B1-%5Cgamma%7D)  
    Then finite!  
    
  - `γ=0`  
    → Agent only cares about the immediate reward.  
    → Short-sighted agent  
  - `γ=1`  
    → Agent takes future rewards into account more strongly  
    → Far-sighted agent  
    
  - `Discounting`식도 Recursive하게 쓸 수 있다!!!  
    ![](https://latex.codecogs.com/gif.latex?G_t%20%5Cdoteq%20R_%7Bt&plus;1%7D%20&plus;%20%5Cgamma%20R_%7Bt&plus;2%7D%20&plus;%20%5Cgamma%5E2%20R_%7Bt&plus;3%7D%20&plus;%20%5Cgamma%5E3%20R_%7Bt&plus;4%7D%20&plus;%20...)  
    ![](https://latex.codecogs.com/gif.latex?%3D%20R_%7Bt&plus;1%7D%20&plus;%20%5Cgamma%20%28R_%7Bt&plus;2%7D%20&plus;%20%5Cgamma%20R_%7Bt&plus;3%7D%20&plus;%20%5Cgamma%5E2%20R_%7Bt&plus;4%7D%20&plus;%20...%29)  
    ![](https://latex.codecogs.com/gif.latex?G_t%20%3D%20R_%7Bt&plus;1%7D%20&plus;%20%5Cgamma%20G_%7Bt&plus;1%7D)  
    
#### Summary  
- In continuing tasks, the agent-environment interaction goes on indefinitely  
- Discounting is used to ensure returns are finite  
- Return can be defined recursively  

### 2.3.2 Examples of Episodic and Continuing Tasks    
- Episodic Task   
  ![](http://drive.google.com/uc?export=view&id=1ARRTcMvpT0CRcnxWV9N_rEnuqg-5j58H)    
  - 해당 게임의 룰: 진 파란색이 하늘색 네모를 먹으면 +1, 초록색에 닿으면 게임 끝  
  - 한 episode가 어떻게 끝나든, 다음 에피소드는 항상 agent가 가운데에 있고, 적은 없는 상황에서 시작.  
  
- Continuing Task  
  - 서버 스케쥴링  
  - state: # of free servers  
  - action: reject or accept 
  - reward: reject(:negative proportional to priority and sends the job to the back of the queue.)  
  - agent는 높은 우선순위의 job이 이후에 처리되는 걸 막기 위해서 저순위의 job 스케쥴링할 때 주의해야.  
  - 이 process는 멈추지 않는다! 즉 continuing task.  
  
#### Summary  
- Episodic tasks break naturally into independent episodes.  
- Continuing tasks are assumed to continue indefinitely.  

  → Should be able to determine which formulation is most appropriate for a given problem.  
  
---  
## Week2 Summary  

```  
- MDPs formalize the problem of an agent interacting with an environment.  
  - The agent and environment interact at discrete time steps.  
  - At each time, the agent observes the current state of the environment.  
  - Based on this state, the agent selects an action.  
  - After that, the environment trasitions to a new state and emits a reward.  
  
  - The agent's choices have long-term consequences.  
    The action it selects influences future states and rewards.  
    
- The goal of Reinforcement Learning  
  - to maximize total future reward  
    (=balancing immediate reward with the long-term consequences of actions)  
    - We formalize this goal with the expected return, which is the  
      expected discounted sum of future rewards.  
        - By discounting with Gamma < 1, can guarantee the return remains finite.  
        - The value of gamma → defines how much care about short-term rewards vs long-term rewards.  

- Example of continuing and episodic MDPs  
  - MDP formalism can be used to model many real-world problems.  
  - The first step in applying rl will always be to formulate the problem as MDP.
```  
