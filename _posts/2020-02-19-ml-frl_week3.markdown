---
layout: post
title:  "Fundamentals of Reinforcement Learning Week3"
subtitle:  "FRL_Week3"
categories: ml
tags: rl
comments: true

---

- 강화학습 및 Multi-Armed Bandit(MAB) Problem에 대한 간단한 소개와, 이를 해결하기 위한 몇 가지 알고리즘에 대한 소개입니다.  

---  
## 3.1 Policies and Value Functions  
### 3.1.1 Specifying Policies  
- agent의 역할: 각 time step마다 action 선택하기  
- action의 선택: 현재의 reward와 다음 state에 영향을 미침  

  ```  
  Almost all rl algorithms involve estimating value functions - functions of states (or of
  state-action pairs) that estimate how good it is for the agent to be in a given state (or
  how good it is to perform a given action in a given state). The notion of 'how good' here 
  is defined in terms of future rewards that can be expected, or, to be precise, in terms of
  expected return. Of course, the rewards the agent can expect to receive in the future 
  depend on what actions it will take. Accordingly, value functions are defined with respect 
  to particular ways of acting, called 'policies'.  
  ```

- Policy  
  = mapping from `states` to `probabilities` of selecting each possible action.  

- Deterministic Policy  
  - A policy maps each state to a `single` action.  
  - `π` ← Policy를 표기  
  - `π(s) = a`: Policy π에 의해 state s에서 a라는 action이 선택되었다.  
  - Agent can select the same action in multiple states, and some sactions might not be selected in any state.  
  ![](http://drive.google.com/uc?export=view&id=12A5GbMkV2uEHCW3U92DNkyI5V3Hf2WOE)  
  
- Stochastic Policy  
  - A policy assigns probabilities to each action in each state.  
  - `π(a|s)`  (![](https://latex.codecogs.com/gif.latex?S_t%3Ds)일 때, ![](https://latex.codecogs.com/gif.latex?A_t%20%3D%20a)일 확률)  
  - Multiple actions may be selected with non-zero probability.  
  ![](http://drive.google.com/uc?export=view&id=1iQmR1xWih-aIaHsftLXfRtZU9HCWVRRQ)  
    - 물론 그림과는 다르게, 각 state마다 취할 수 있는 action들이 다를 수도 있다.  
    
- Policy: 오직 `현재`의 state에만 의존한다.  

#### Summary  

```  
- A policy maps the current state onto a set of probabilities for taking each action  
- Policies can only depend on the current state  
```  

### 3.1.2 Value Functions  
- 예를 들어, 가게에 재고가 많을 때  
  - 단기 수익을 극대화 하기 위해 전체 재고를 싸게 팔아버리거나.  
  - 수요가 늘어났을 때까지 보관했다가, 수요가 늘어났을 때 팔거나.  
  
  - reward: 전자 (short-term gain에 집착)  
  
    ```  
    아 그러네. 그냥 각 time에서의 R (reward)들은 한 시점에 대해 얻어진 값이니까.  
    그렇다면, value function은 State(혹은 (State, Action) pair))에 따른 장기적인 평가을 위한 조치라고 볼 수 있겠네.
    ```  
    
  - `But`, The objective is to learn policy that achieves the most reward `in the long run`.  
  
    ```  
    Value functions formalize what this means!!!  
    ```  
    
- `State value function`  
  - The future award an agent can expect to receive starting from a particular state.  
    = `Expected return from a given state`  
    ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D%28s%29%20%5Cdoteq%20E_%7B%5Cpi%7D%5BG_t%7CS_t%3Ds%5D)  
    (![](https://latex.codecogs.com/gif.latex?G_t%20%3D%20%5Csum_%7Bi%3D0%7D%5E%7B%5Cinfty%7D%5Cgamma%5EkR_%7Br&plus;k&plus;1%7D))  
    - 좌항: `Value function is contingent on the agent selecting actions according to policy π`  
    - 우항: `Expectation is computed with respect to policy π`  
    
- `Action value function`  
  - What happens when the agent first selects a particular action  
    = `expected return if the agent selects action A and then follows policy π`  
    ![](https://latex.codecogs.com/gif.latex?q_%7B%5Cpi%7D%28s%2Ca%29%20%3D%20E_%7B%5Cpi%7D%5BG_t%7CS_t%3Ds%2C%20A_t%3Da%5D)  
  - 어떤 State s에서 Action a를 취할 경우의 받을 return에 대한 기대값!  
    즉, 어떤 행동을 했을 때 얼마나 좋을 것인가에 대한 값  

    - `"Action value function을 사용하면 value function과는 달리 단지 어떤 행동을 할지 action value function의 값을 보고 판단하면 되기 때문에 다음 state들의 value function을 알고 어떤 행동을 했을 때 거기에 가게 될 확률도 알아야하는 일이 사라집니다."`  
    
  
- Value function의 장점   
  - 1. The return is not immediately available  
  - 2. The return may be random due to stochasticity in both the policy and environment dynamics.  
  
  - Summarizes all the possible futures by averaging over returns  
  - 각기 다른 policy들의 퀄리티를 판단할 수 있도록 해준다.  
  
  ● Episodic MDP case  
  
    ```  
    예: 체스 게임  
    - reward가 가령, 이기면 +1, 지면 0
    - 하지만 이것이, agent가 체스를 플레이 하는 동안에 얼마나 잘 하고 있는지를 표현해주지는 못해  
      (게임이 끝날 때까지 기다려 봐야 앎)   
    ```  
    
    ↕ Value Function은 더 많은 것을 말해줄 수 있다.  
    
    ```  
    - State value = Expected sum of future rewards.  
      : 체스게임의 경우 → The probability of winning if we follow the current policy π
        (왜냐하면 0이 아닌 reward의 유일한 경우는 이기는 것 뿐이기 때문)  
    ```
    
    ![](http://drive.google.com/uc?export=view&id=1ZVab4NiYW6o0kAUCDY2ljJLQ1sYZdusG)  
    
    ```  
    - state: S → S'  
    - Value of S' < Value of S 
      : Policy π를 따랐을 때에 새로운 state S'에서 state S보다 이길 확률이 낮아졌다.  
    ```  
    
  ● Continuing MDP case  
  - continuing task: specify γ  
  - Value function compactly summarizes all the possibilities.  
  
#### Summary  

```  
- State-value functions represent the expected return from a given state under a specific policy   
- Action-value funcitons represent the expected return from a given state after taking a specific action, 
  later following a specific policy  
```  

## 3.2 Bellman Equations  
### 3.2.1 Bellman Equation Derivation  
- `Bellman equation` formalizes the connection between the value of a state and its possible successors.  

- `A fundamental property of value functions used throughout rl and dynamic programming is that they satisfy recursive relationships.`    
  
- State-value Bellman equation  
  ![](http://drive.google.com/uc?export=view&id=1bicWtlQMkSGCHDHD4DdD7dW0ZEhfx6jr)  
  ![](http://drive.google.com/uc?export=view&id=1PplmZXMQgwInIllBhr-I37_e_3MyJrGs)  
  ![](http://drive.google.com/uc?export=view&id=1Jn5oIW-SXi8sLIzaghNq4VBTna7K_Iiq)  
  - State value function: expected return starting from state S  
  - Return: the discounted sum of future rewards  
    (Return at time t can be written recursively as the immediate reward + discounted return at time t+1)  
  
  - First, expand the expected return as a sum over possible action choices made by the agent.  
  - Second, expand over possible rewards and next states condition on state S and action a.  
  
  - ![](https://latex.codecogs.com/gif.latex?R_%7Bt&plus;1%7D): Random variable  
  - r: each possible reward outcome  
  - Expected return depends on states and rewards infinitely far into the future.  

  - `Bellman Equation` for State-value    
  ![](http://drive.google.com/uc?export=view&id=1Jn5oIW-SXi8sLIzaghNq4VBTna7K_Iiq)  
  
- Action-value Bellman equation  
  ![](http://drive.google.com/uc?export=view&id=1qxumEyuSUOsxoCiI_7YhvVWw3D2S1Sya)    
  ![](http://drive.google.com/uc?export=view&id=116ri8W5BIqxHeCcjsIKKtBReIja6cuZs)  
  - recursive equation for the value of a state-action pair in terms of its possible  
    successors state-action pair  
  - `The equation does not begin with the policy selecting an action.`  
    (∵ The action is already fixed as part of the state-action pair)  
    - Instead, skip directly to the dynamics function p to select the immediate reward and  
      next state S'.  
  ![](http://drive.google.com/uc?export=view&id=1PGpOfJphb-ldZR8BAiZlAV54z3LkUr1b)  
  
  - `Bellman Equation` for Action-value  
  ![](http://drive.google.com/uc?export=view&id=116ri8W5BIqxHeCcjsIKKtBReIja6cuZs)  
    - recursive equation for the value of one state-action pair in terms of the  
      next state-action pair  
      
 #### Summary  
 
 ```  
 - Bellman Equations for state-value and action-value functions.  
 - The current time-step's state/action values can be written recursively 
  in terms of future state/action values  
```  

### 3.2.2 Why Bellman Equations?  
- Bellman Equation  
  : Allowed us to express the value of a state, or state-action pair in terms of its possible successors → 그래서 뭐?  

- 예시로 시작,  
  ![](http://drive.google.com/uc?export=view&id=1f3rAfgzt_y1SEJT8jaOpoYVaSuhiKyQF)  
  
  
  4 States: A,B,C,D  
  Actions: 위, 아래, 왼쪽, 오른쪽 이동  
  벽에 부딪힐 경우 제자리.  
  State B에 도착하면 reward +5, 나머지는 다 +0 (B에서 계속 벽에 부딪히는 경우까지 포함)  
  
  Uniform random 'policy' 가정 (매번 각 direction으로 움직일 확률: 25%)
  discount factor γ = 0.7  
  
  ![](http://drive.google.com/uc?export=view&id=1ahYVW4bzbbs2zsAyGn6ON7GnvSv8wZyY)  
    
  - (Recall) Value function: expected return under policy π
    - Bellman equation: 이에 대해 굉장히 우아한 솔루션 제공  
    

  ![](http://drive.google.com/uc?export=view&id=1G5DNnNjlNfzYYTNMUXCrhXttRHYmkFA8)  
  ![](http://drive.google.com/uc?export=view&id=1E2ztunOkIL-uKWJdiDKpAhg1RwNOjNuq)  
  ![](http://drive.google.com/uc?export=view&id=1eOUZBOMqS4uoan3LVY-iIC4OVj_54ajN)  
  
  - Bellman equation reduce an unmanageable infinite sum over possible futures  
    to a simple linear algebra problem.  
  - Bellman equation provides a powerful general relationship for MDPs.  
  
  - Bellman equation may be possible for MDPs of moderate size.  
  - 하지만, 이보다 더 복잡한 문제에서는, `directly writing down a system of equations`  
    `for the state values and solve the systems to find values` 이것이 안 먹힐 것  
    - 체스: 가능한 state수만 해도 10^45 정도. 이걸 다 listing하기란 가능하지 않을 것  
    
#### Summary  

```  
- You can use the Bellman Equations to solve for a value function by
  writing a system of linear equations
- We can only solve small MDPs directly, but Bellman Equations will
  factor into the solutions we see later for large MDPs.  
```  

---  

## 3.3 Optimality (Optimal Policies & Value Functions)  
### 3.3.1 Optimal Policies  
- RL의 목적은 단순히 특정 policy를 평가하는 것만이 아니다.  
  - RL의 궁극적 목적: `Finding a policy that obtains as much reward as possible in the long run.`  

- 이 강의에서 배울 것  

  ```  
  - Define an optimal policy  
  - Understand how a policy can be at least as good as every other policy in every state  
  - Identify an optimal policy for a given MDP  
  ```  
  
- 한 policy가 다른 policy보다 낫다는 것이 무엇인가?  

  ![](https://latex.codecogs.com/gif.latex?%5Cpi_1%20%5Cgeq%20%5Cpi_2)  
  : `모든` state에 대하여 policy π1에서의 value (expected return)가 높을 때에`만`(`iff`) 이렇게 얘기할 수 있음  
  
- Optimal policy(![](https://latex.codecogs.com/gif.latex?%5Cpi_*))  
  : The policy as good as or better than all the other policies  
  - 항상 하나 이상의 optimal policy가 존재한다. (둘 이상일 수도 있다)  ← `Proved`  
  ![](http://drive.google.com/uc?export=view&id=1wPj7YG78erNrzIsGxRJMQiW9mj3N66Js)  
  (이 그래프의 x축과 y축이 무엇인지를 잘 보자~)  
  
  ![](http://drive.google.com/uc?export=view&id=1jFJuXS7jIYZNQEVhhE6_M1pH2zlrqYBn)  
  - γ = 0 → π1: optimal policy  
  - γ = 0.9 → π2: optimal policy  
  
  - 위의 경우: Policy가 두 개였고, 각각에 대해 value function을 계산.  
    - 하지만 일반적으로, 이는 쉽지 않다.  
    
      ```  
      Even if we limit ourselves to deterministic policies, the # of possible policies is 
      equal to the # of possible actions to the power of the # of states.  
      ```   
      
    - 뒤에서, `Bellman Optimality Equation`를 소개할 것  
    
#### Summary  
```  
- An optimal policy is defined as the policy with the highest possible value function 
  in all states  
- At least one optimal policy awlays exists, but there may be more than one
- The exponential number of possible policies makes searching for the optimal policy 
  by brute-force(무차별 대입) intractable(다루기 힘든). 
```  

### 3.3.2 Optimal Value Functions  

- Optimal Value Functions  
  - Recall that  
  ![](https://latex.codecogs.com/gif.latex?%5Cpi_1%20%5Cgeq%20%5Cpi_2%20%5C%3B%20iff%20%5C%3B%20v_%7B%5Cpi_1%7D%28s%29%20%5Cgeq%20v_%7B%5Cpi_2%7D%28s%29%20for%20%5C%3B%20all%20%5C%3B%20s%20%5Cin%20S)  
  
  ```  
  For two policies π1, π2, π1 is considered as good as or better than π2 iff 
  the value under π1 is greater than or equal to the value under π2 for all states.  
  
  An optimal policy is one which as good as or better than every other policy.  
  The value function for the optimal policy thus has the greatest value possible in every state.  
  ```  
  
  ## ![](https://latex.codecogs.com/gif.latex?%5Cpi_*)  
  - Optimal policy (하나 이상이 존재할지라도)  
  - Optimal policies: share the `same` `state-value functions` & `action-value functions`  
    - 그 때의 state-value function: `optimal state-value function` ![](https://latex.codecogs.com/gif.latex?v_%7B*%7D%28s%29)   
    - 그 때의 action-value function: `optimal action-value fuction` ![](https://latex.codecogs.com/gif.latex?q_*%28s%2Ca%29)  
  
  
  ## ![](https://latex.codecogs.com/gif.latex?v_*)  
  ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi_*%7D%28s%29%20%5Cdoteq%20E_%7B%5Cpi_*%7D%5BG_t%7CS_t%3Ds%5D%20%3D%20max_%7B%5Cpi%7Dv_%7B%5Cpi%7D%28s%29%20%5C%3B%20for%20%5C%3B%20all%20%5C%3B%20s%5Cin%20S)  
  - All optimal policies have the same optimal state-value function, which is denoted by ![](https://latex.codecogs.com/gif.latex?v_*)  
  ## ![](https://latex.codecogs.com/gif.latex?q_*)  
  ![](https://latex.codecogs.com/gif.latex?q_%7B%5Cpi_*%7D%28s%2Ca%29%20%3D%20max_%7B%5Cpi%7Dq_%7B%5Cpi%7D%28s%2Ca%29%20%5C%3B%20for%20%5C%3B%20all%20%5C%3B%20s%20%5Cin%20S%20%5C%3B%20and%20%5C%3B%20a%5Cin%20A)  
  - Optimal policies also share the same optimal action-value function,  
    which is again the maximum possible for every state action pair.  
  - state-action 페어 (s,a)에 대해, 이 함수는 state s에서 action a를 취했을 때의 expected return을 반환, and then thereafter following an optimal policy.  
  ![](https://latex.codecogs.com/gif.latex?q_*%28s%2Ca%29%20%3D%20E%5BR_%7Bt&plus;1%7D%20&plus;%20%5Cgamma%20v_*%20%28S_%7Bt&plus;1%7D%29%7CS_t%3Ds%2C%20A_t%3Da%5D)  
  
  
  ↓  
  ↓  
  
  - Recall that  
  ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D%28s%29%20%3D%20%5Csum_a%5Cpi%28a%7Cs%29%5Csum_%7Bs%27%7D%5Csum_rp%28s%27%2Cr%7Cs%2Ca%29%5Br%20&plus;%20%5Cgamma%20v_%7B%5Cpi%7D%28s%27%29%5D)  
  
  ## ![](https://latex.codecogs.com/gif.latex?v_*)  
  ![](https://latex.codecogs.com/gif.latex?v_%7B*%7D%28s%29%20%3D%20%5Csum_a%5Cpi_%7B*%7D%28a%7Cs%29%5Csum_%7Bs%27%7D%5Csum_rp%28s%27%2Cr%7Cs%2Ca%29%5Br%20&plus;%20%5Cgamma%20v_%7B%5Cpi%7D%28s%27%29%5D)  
  - 이것은 optimal policy이기 때문에 아래와 같이 다시 표현해볼 수 있음  
  ![](https://latex.codecogs.com/gif.latex?v_%7B*%7D%28s%29%20%3D%20max_a%20%5Csum_%7Bs%27%7D%5Csum_r%20p%28s%27%2Cr%7Cs%2Ca%29%5Br&plus;%5Cgamma%20v_*%28s%27%29%5D)  
  `π* no longer appears in the equation`  
  
  ```  
  Because it is the optimal value functio, however, v*'s consistency condition can be written in a special 
  form without reference to any specific policy.
  ```  
  
  ```  
  Remember, there always exists an optimal deterministic policy, 
  one that selects an optimal action in every state.  
  Such a deterministic optimal policy will assign probability 1, 
  for an action that ahcieves the highest value, 
  and probability 0, for all other actions.  
  ```  
  
  `Have derived a relationship that applies directly to v* itself`  
  
  ![](https://latex.codecogs.com/gif.latex?v_%7B*%7D%28s%29%20%3D%20max_a%20%5Csum_%7Bs%27%7D%5Csum_r%20p%28s%27%2Cr%7Cs%2Ca%29%5Br&plus;%5Cgamma%20v_*%28s%27%29%5D)  
  : Bellman Optimality Equation for ![](https://latex.codecogs.com/gif.latex?v_*)  
  
  
  ## ![](https://latex.codecogs.com/gif.latex?q_*)  
  
  ![](http://drive.google.com/uc?export=view&id=1YN69IAN31VbUU9SAmXQAnk8aEWrJp_4q)  
  ![외부 링크](https://dnddnjs.gitbooks.io/rl/content/3334.png)  
  ![](http://drive.google.com/uc?export=view&id=1caMwsj0khcils7KH4kF6quOcRVjdNORG)  
  
  ```  
  - 앞전에, Bellman equation을 system of linear equation을 통해 해결.  
  - (a)  
    하지만, max는 linear operation이 아니기 때문에 system of linear equation으로 해결 불가.  
    ↑
    한가지 의문이, "그럼, 일반적인 Bellman Eqation에서 π를 π*로 대체하면 되잖아요!" (b)
    No. 우리는 π*를 모른다.  알았다면 강화학습의 목표 이미 달성하고 남았을 것   
  ```  
  
    `v*에 대해 Bellman optimality equation을 해결할 수 있다면, π*를 꽤 쉽게 얻을 수 있을 것`  
    
#### Summary  
```   
- We inroduced optimal value functions and derived the associated Bellman optimality equations.  
- The Bellman optimality equations relate the value of a state, or state-action pair, 
  to its possible successors under any optimal policy
```  


### 3.3.3 Using Optimal Value Functions to Get Optimal Policies  
- 음, 근데 우리가 원하는 건 optimal policy 그 자체를 찾는 건데,  
  optimal policy의 value function을 찾는 게 왜 중요한가? 라는 의문.  


![](http://drive.google.com/uc?export=view&id=1zc0K2v_-IpONlRkhpcZMdgxB6ZHUd2z_)  
- 일반적으로, v* 를 안다는 것은 곧, 우리가 `dynamic funciton` p에 대해 아는 한,  
  optimal policy를 얻어내기가 상대적으로 쉽다는 것.  
- 어떤 state에서든, 사용가능한 action을 알 수 있고, 위 식의 box term을 평가할 수 있음  
  - 분명 이 box term을 최대로 만드는 action이 있을 것  
  - 각 state에 대하여 maximizing action을 택하게 하는 policy가 바로 `optimal`일 것  
    (A deterministic policy which selects this maximizing action for each state  
    will necessarily be optimal, since it obtains the hightes possible value.)  
  - `The equation shown here for π* is thus almost the same as the Bellman optimality equation for v*.`    
  - v* is equal to the maximum of the boxed term over all actions.  
  - π* is the argmax, which simply means the particular action which achieves this maximum.  
  
- 주어진 action에 대해 boxed term을 평가하려면, 단지 다음 state로 가서 그에 따르는 reward를 보면 됨  

  ```  
  - A1이라는 particular action  
  - A1이라는 action을 취하고 난 뒤, 각 state들과, 그 state에 뒤따르는 reward를 봄.  
    - v*와 p에 대해 알 수 있기 때문에, s'와 r의 sum 형태로 각 항들을 평가할 수 있다.  
  - A1에 대해 boxed term이 5로, A2에 대해 10으로, A3에 대해 7로 ...  
  - A1, A2, A3라는 세 개의 action들 중, A2가 10이라는 값으로 boxed term을 최대화하므로, 
    A2가 곧 'optimal action'.  
    
  - optimal action이 여러 개 존재할 수도 있다.  
    - stochastic optimal policy를 정의  
      (일정 확률로 그들 사이에서 선택)  
  ```  
  
- 예시    
  ![](http://drive.google.com/uc?export=view&id=1c_tHihtJlJ2KLWvqWp6WBoBq8ujtPg0t)  
  
  ```  
  -1 + 0.9*16.0 = 13.4
  0 + 0.9*14.4 = 13.0
  0 + 0.9*17.8 = 16.0  
  ```  
  
  ![](http://drive.google.com/uc?export=view&id=1owGn9YNxexdugUS4DHAcnhBgyZ8SmECy)  
  
  ```  
  14.0, 13.4, 13.0, 16.0 사이에서 가장 큰 값은 16.0 (←: going left)
  - Left is the optimal action in this stat, and must be selected by any optimal policy.  
  - For the maximixing left(←) action, the right side of the equation of value is the 16 
    which is indeed equal to v star for the state itself.  
  ```  
  
  ![](http://drive.google.com/uc?export=view&id=1yNmFR-R36eVkgiqX0RJ6_SYukx0rPd0x)  
  - 이 state의 경우에는, 두가지 다른 액션이 같은 optimal value를 낳고 있어 (17.8)  
  - An optimal policy is free to pick either with some probability.  
  
  ![](http://drive.google.com/uc?export=view&id=1fUUjFYsMWm8nARLTdveR6TtSnv3JrX3H)  
  - state A의 경우에는 어떤 action을 선택하든지 간에, A'을 가는 게 최고  
  - `Every action is optimal since the transitions are equivalent`  
  - `v* for state A is 24.4 (recorded value for v star)
  
  ![](http://drive.google.com/uc?export=view&id=1mDfA3v1xH-fEocEYJGo8kam4mgkChFq4)  
  - `Optimal policy essentially heads toward state A to obtain +10 reward as quickly as possible.`  
  
  ![](http://drive.google.com/uc?export=view&id=1o809TDK7vL-OhVJDGoO4kh1jjOa1bCHJ)  
  
  ```  
  Working out the optimal policy from v* is especially simple in this grid world.
  Each action leads us deterministically to a specific next state and reward. 
  So we only have to evaluate one transition per action. 
  ```  
  
  - `dynamics function` p는 확률적이라 (`stochastic`), 항상 그리 단순하지만은 않다.  
  - (1) 하지만, p에 대해 접근할 수 있는 한, v* 로부터 항상 optimal action을 찾아낼 수 있다.  
    (by computing the right-hand side of teh Bellman optimality equation for each action and  
     finding the largest value.)  
  - (2) 대신, q* 에 대해 접근 가능하다면, optimal policy 찾기는 훨씬 쉬워진다.  
    - optimal policy를 찾기 위해 한 발자국 더 나아가볼 필요 없다.  
    -그냥 q* 를 최대화하는 어떤 action a를 찾기만 하면 된다.    
    
  ```  
  The action-value function caches the results of a one-step look ahead for each action.  
  In this sense, the problem of finding an optimal action-value function corresponds to the goal
  of finding an optimal policy. 
  ```  
  
#### Summary  

```  
- Once we have the optimal state value function, it's relatively easy to work out
  the optimal policy.  
- If we have the optimal action-value function, working out the optimal policy
  is even easier. 
```  

---  

### Summary  
- Policies tell an agent how to behave in their environment.  
  - Deterministic policy  
  
    ```  
    - Deterministic policies map each state to an action.  
    - Each time a state is visited, a deterministic policy selects the associdated action π of S.
    ```  
    
  - Stochastic policy  
  
    ```  
    - Stochastic policies map each state to a distribution over all possible actions.  
    - Each time state is visited, a stochastic policy randomly draws an action from the associated
      distribution with probability π of A given S.  
    ```  
    
  - A policy, by definition, depends only on the `current state`.  
    (Not on time or previous states.)  
     → It is a restriction on the `state`, not the agent`.    
     - The state should provide the agent with all the information it needs to make a good decision.  
     
- Value functions estimate future return(total reward) under a specific policy.  
  - State-value functions  
    ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D%28s%29%20%5Cdoteq%20E_%7B%5Cpi%7D%5BG_t%7CS_t%3Ds%5D)  
    - expected return from the current state under a policy  
  - Action-value functions  
    ![](https://latex.codecogs.com/gif.latex?q_%7B%5Cpi%7D%28s%2Ca%29%20%5Cdoteq%20E_%7B%5Cpi%7D%5BG_t%7CS_t%3Ds%2C%20A_t%3Da%5D)  
    - expected return from state S if the agent first selects actions A and follows π after that.  

  - Value functions simplify things by aggregating many possible future returns into a single number.  
  
- Bellman Equation define relationship between the value of a state or state-action pair  
  and its successor states.  
  ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D%28s%29%20%3D%20%5Csum_a%20%5Cpi%28a%7Cs%29%5Csum_%7Bs%27%7D%20%5Csum_r%20p%28s%27%2Cr%7Cs%2Ca%29%5Br%20&plus;%20%5Cgamma%20v_%7B%5Cpi%7D%28s%27%29%5D)  
  ![](https://latex.codecogs.com/gif.latex?q_%7B%5Cpi%7D%28s%2Ca%29%20%3D%20%5Csum_%7Bs%27%7D%5Csum_r%20p%28s%27%2Cr%7Cs%2Ca%29%5Br%20&plus;%20%5Cgamma%20%5Csum_%7Ba%27%7D%5Cpi%28a%27%7Cs%27%29q_%7B%5Cpi%7D%28s%27%2Ca%27%29%5D)  
  
  - Bellman Equation for State-value function  
    - gives the value of the current state as a sum over the values of all the successor states,  
      and immediate rewards.  
  - Bellman Equation for Action-value function  
    - gives the value of a particular state-action pair as the sum over the values of all possible    
      next state-action pairs and rewards.  
      
  - Bellman Equation can be directly solved to find the value function.  
 
- Bellman Equation:  
  help us evaluate policies, but didn't achieve goal yet.  
- Goal: finding a policy that obtains as much reward as possible   
  - Optimal policies  
    - policy which achieves the highest value possible in every state (than any other policy).  
    - at least one!!!  
  - The optimal value function  
    - optimal state value function  
      = the highest possible value in every state  
      
    ```  
    - Every optimal policy shares the same optimal state-value function.  
    - Every optimal policy shares the same optimal action-value function.  
    ```  
    
  - Bellman optimality equations  
    - Like all value functions, the optimal value functions have Bellman equations.  
    ![](http://drive.google.com/uc?export=view&id=1q-dwhLUBDGco-3-RngS8t6qpayNl9QFc)  
    ↓  
    ↓  These Bellman equations do not reference a specific policy.  
    ↓  This amounts to replace in the policy in the Bellman equation with a max over all actions.  
    ↓  
    ![](http://drive.google.com/uc?export=view&id=1QvtIFMP5zdKLp1dSl7stybgqLebVVAD_)  
    - The optimal policy must always select the best available action.  
    - We can extract the optimal policy from the optimal state value function.  
    - But to do so, we also need the one-step dynamics of the MDP.  
    - We can get the optimal policy with much less work if we have the optimal action function.  
      - Simply select the action with the highest value in each state.  
