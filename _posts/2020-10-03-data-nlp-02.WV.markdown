---
layout: post
title:  "CS224N - 02.Word Vectors and Word Senses"  
subtitle:   "GNN-02.WVWS"
categories: data
tags: nlp
comments: true
---

- CS224N의 2주차 강의, Word Vectors and Word Senses를 보고 정리한 글입니다.  
  [1. Word2vec Review](#word2vec-review)  
  [2. Optimization Basic](#optimization-basic)  

---  

## Word2vec Review  
- Idea  
  - "Iterative updating algorithm" (전체 corpus의 각 단어들을 iterate하라.)  
  - 전체 word vectors를 이용하여 주변 단어 예측하기  
    ![nlp02-1](https://user-images.githubusercontent.com/43376853/94986902-707a4e00-059d-11eb-8f40-0389eecdad3e.png)  
    ![](https://latex.codecogs.com/gif.latex?%5Cdpi%7B120%7D%20P%28o%7Cc%29%20%3D%20%5Cfrac%7Bexp%28u_0%5ETv_c%29%7D%7B%5Csum_%7Bw%20%5Cin%20V%7Dexp%28u_w%5ET%20v_c%29%7D)  
    ▲ 중심단어 c가 나타났을 때, 주변단어 o가 나타날 확률  
  
### Word2vec parametes and computations  
- PyTorch, Tensorflow등 모든 딥러닝 패키지들: Word vector들을 Row로 표현한다! (일반적으로 수학에서 Column으로 생각하는 사고랑 다름)  
  ![nlp02-2](https://user-images.githubusercontent.com/43376853/94986990-2776c980-059e-11eb-8846-4c5952e7fc35.png)  
  
  > 'Word2vec maximizes objective function by putting similar words nearby in space'  
  
## Optimization Basic   
### Gradient Descent  
- J(Θ)을 Minimize하기!  
  (물론, 고차원 Space에서는 아래와 같은 Convex function이 아니라 Horrible한 Non-convex cost일 때도 많음)   
  ![nlp02-3](https://user-images.githubusercontent.com/43376853/94987133-dd421800-059e-11eb-875c-166fe4279725.png)  
- Update Rule  
  ![nlp02-4](https://user-images.githubusercontent.com/43376853/94987137-de734500-059e-11eb-8eb6-8d3edf6d55e9.png)  
  
- Code  

  ```  
  while True:
    theta_grad = evaluate_gradient(J, corpus, theta)
    theta = theta - alpha * theta_grad
  ```  
 
### Stochastic Gradient Descent   
- 한편, 한 번에 다 Update하는 것은 상당히 Ineffective  
- Solution: SGD 사용 (Windows를 반복적으로 sampling & update)  
  
  ```  
  while True:
    window = sample_window(corpus)
    theta_grad = evaluate_gradienet(J, window, theta)
    theta = theta - alpha * theta_grad
  ```  
  
  (다만 mini-batch Gradient Descent가 Stochastic Gradient Descent보다 Effective 한 경우도 많음.)  
  (이유1. SGD보다 less noisy, 이유2. GPU사용시 Parallelization에 good)  
  
- SGD에도 문제점 존재.  
  : 각 window마다, 기껏해야 2m+1 단어를 갖기 때문에,  
    ![](https://latex.codecogs.com/gif.latex?%5Cdpi%7B120%7D%20%5Cbigtriangledown%20_%5Ctheta%20J_t%28%5Ctheta%29) 는 상당히 Sparse!  
    ![nlp02-5](https://user-images.githubusercontent.com/43376853/94987286-38283f00-05a0-11eb-9510-542026c4fa90.png)  
  : 이 말인 즉슨, 실제로 나타나는 단어들에 대해섬나 word vector를 업데이트하게 될 것이라는 것.  
  
  - Solution: Word vectors에 대한 hash 들고 다니기  
  
### Word2vec: More details  
- 두 개의 벡터를 들고 다니는 이유?  
  - 최적화가 더 쉽다! (1과에 식 이미 정리)   
  
- 두 가지 Variants  
  - Skip-grams  
    : Center world 1개로 주변 단어들 한 번에 예측하기  
  - Continuous Bag of Words (CBOW)  
    : 그 반대  
    
- Training의 효율을 더하는 방법  
  : By __Negative Sampling__  (이제껏 집중해온 방식은 __naive softmax__)  
  
  
### The skip-gram model with negative sampling  
- 이제까지 써오던 Probability notation  
  ![](https://latex.codecogs.com/gif.latex?%5Cdpi%7B120%7D%20P%28o%7Cc%29%20%3D%20%5Cfrac%7Bexp%28u_0%5ETv_c%29%7D%7B%5Csum_%7Bw%20%5Cin%20V%7Dexp%28u_w%5ET%20v_c%29%7D)  
  는 속도면에서 상당히 좋지 않은 아이디어. (이 확률값 하나 계산하려고, 전체 vocab을 다 훑어야함)  
- Standard word2vec 대신 skip-gram model with __negative sampling__ 을 생각해보자.  
- 핵심 아이디어: 
  
  
---  
#### References  
[CS224N: Word Vectors and Word Senses](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture02-wordvecs2.pdf)  
