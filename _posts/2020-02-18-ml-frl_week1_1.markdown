---
layout: post
title:  "Fundamentals of Reinforcement Learning Week1"
subtitle:  "FRL_Week1"
categories: ml
tags: rl
comments: true

---

- 강화학습 및 Multi-Armed Bandit(MAB) Problem에 대한 간단한 소개와, 이를 해결하기 위한 몇 가지 알고리즘에 대한 소개입니다.  

---  

## 1.1 Course Introduction  

### 1.1.1 Specialization Introduction  

- Situation  
  - 팀원들이 새 알고리즘 테스트하는데 밤들 새면서 이것저것 먹다보니 랩실이 너무 더러워짐  
  - 그래서 로봇으로 하여금 청소시킬 생각  `How?`  
  
  - 강화학습  
    - Reward: 로봇이 모으는 캔 수  
    - Agent→ 많은 `Train and Error`를 통해 최대의 캔을 모으는 방법 학습  
    - 원칙적으로 랩실의 '지도'를 알 필요도 없음  
      - 누가 랩실의 가구 배치를 달리해도 그 또한 스스로 자동으로 학습  
      - 따라서,  
      
        ```  
        - good exploration method  
        - function approximation (if we want to learn from an on-board camera)
        필요  
        ```  
        
    - Reward Function: 하루의 끝에 로봇이 모은 캔을 세어보도록 할 수 있겠다.  
      - 따라서 이 delayed feedback을 잘 handling하기 위해 `TD-based algorithm`을 필요로 할 것  
        - cf) [TD(Temporal Difference) algorithm](https://www.google.com/search?q=td+algorithm&oq=td+algori&aqs=chrome.1.69i57j0l5.7662j0j4&sourceid=chrome&ie=UTF-8)   
      
    - 이 코스는 이 모든 개념들을 다룰 것  
      
      
### 1.1.2 Course Introduction  
 
- RL: not that old idea  
- 지도학습, 비지도학습, 강화학습 차이?  
  - 지도학습: 라벨있다는 가정  
    - 선생님이 학생에게 정답을 주는 교육법  
  - 비지도학습: 데이터의 기저에 있는 구조를 추출 (extracting underlying structure in data)  
    - data representation  
    - 비지도학습이 지도학습이나 강화학습의 성능 향상에 도움이 될 수 있음  
  - 강화학습: Reward가 agent로 하여금 최근의 action이 얼마나 좋은지 나쁜지에 대한 아이디어를 줌  
    - 선생님이 학생에게 좋은 행동이란 무엇인지는 알려는 주지만 어떻게 그렇게 행하는지에 대한 구체적인 답은 주지 않는 교육법  
    - 지도학습, 비지도학습 기법들이 RL의 일반화를 돕는 데에 사용될 수 있다.  
        
- 세상은 static하지 않다.  
  - 따라서 세상을 완전히 'memorize'하는 것보다 최근의 변화들을 잘 통합하려는 노력 중요  
    → 강화학습  
  - 진짜 어려운 부분은 데이터로부터 무언가를 학습하는 것이 아니라 `online-learning`  
  - 강화학습 분야도 굉장히 빠르게 발전하고 있음.  
    - 하지만 이 빠른 발전 속에서 더 중요한 것은 기본에 충실하는 것  
    
- 강화학습의 한 케이스인 `multi-arm bandits`로부터 강의 시작  
  - `Agent는 어떤 결정이 평균적으로 가장 좋은 결과(outcome or reward)를 낳는지를 결정해야 함`  

- introduction to estimating values, incremental learning, exploration, non-stationarity, parameter tuning
 
### 1.1.3 Meet your instructions!  
- 강사들: 대단한 사람들!  

- 강화학습 왜 중요?  
  - RL이 줄 영향력 크다고 생각 but 여전히 해결해야 할 과제는 많음  
    - RL이 사용되고 있는 영역은 아직은 그리 많지 않음  
    - Robust한 RL 알고리즘으로 좀 더 개선시켜 나가야 할 필요성.  
  - `general approach to automated decision-making`이라는 생각  
  
### 1.1.4 Your Specialization Roadmap  
- 강화학습도 새로운 알고리즘이나 새로운 application이 매주 쏟아져 나와  
- 하지만 강화학습의 기초는 거의 같음  
  ex) DQN  
  
  ```  
  - Q-learning
  - Epsilon-greedy action selection
  - Neural network function approximation, 
  - a few other ideas to achieve superhuman scores in Atari games  
  
  → 이 아이디어들을 합쳐놓은 것  
  ```  
  
  - `DQN`이라는 하나의 learning system에서 강화학습 시스템의 가장 기본적인 초석들을 볼 수 있음  
 
---   
 
## 1.2 The K-Armed Bandit Problem  
### 1.2.1 `Sequential decision making with evaluative feedback`  

- 강화학습
  - agent가 세계와 상호작용하면서 자기 자신의 training data를 생산  
  - 누군가로부터 정답을 입력받지 않고, agent 스스로가 trial and error를 통해 자신의 행동의 결과를 학습  
  
  ```  
  - 예를 들어, 의사가 환자에게 3개의 약 중 하나를 랜덤하게 처방  
  - 곧 의사는 C약이 그 증상에 잘 받는 약이라는 걸 알아차게 됨  
  - 그럼 C약으로 계속 처방할 것인가, 아니면 세 약(A,B,C)을 랜덤하게 처방하는 task를 계속 할 것인가  
    - C약으로 계속 처방할 경우: 나머지 두 약(A,B)에 대한 데이터를 더 이상 모을 수 없다.  
    - 나머지 두 약(A,B) 중 한 개가, C약 보다 오히려 효과가 더 좋을 수도 있다. 현재까지의 'C약이 효과가 제일좋다.'라는 결과는 우연에 의한 것일 수 있다.  
    - 또, 반면 실제로 A,B약이 더 안좋을 경우, 이 실험을 계속하는 것은 피실험 환자들의 건강을 악화시키는 일  
  ```  
  
  → 이 예: 
  - `Decision-making under uncertainty`의 예이자 `K-armed bandit problem`의 예  
  
- `K-armed bandit`  
  ```  
  In the k-armed bandit problem, we have an agent who chooses between 'k'
  actions and receives a reward based on the action it choose  
  
  (Objective: to maximize the expected total reward over some time period (time steps))
  ```
  
### 1.2.2 K-Armed Problem  
- Agent(`Decision Maker`)  
  - k개의 action들 사이에서 선택 & 선택한 액션에 근거하여 reward를 받음  
  
  ```  
  약 처방의 예에서, 
  agent: doctor  
  action: 이 세 약 중 어떤 약을 처방할 것인가 
  reward: 특정 약을 복용하고 난 뒤의 환자의 상태  
  
  * Doctor는 최상의 Reward를 낳는 action을 선택할 것  
  ```  
  
  ```  
  의사가 어떤 action이 가장 좋은 action인지를 고르기 위해 각 action이 갖는 가치를 정의해야 한다.  
  → Action value (Action value function)  
  ```  
    
  
- `Action value`  
  - value of taking each action  
  - 확률의 개념을 사용  
  - expected reward  
  
    ![](https://latex.codecogs.com/gif.latex?q_*%28a%29%5Cdoteq%20E%5BR_t%7CA_t%3Da%5D%2C%20%5Cforall%20a%20%5Cin%20%5Cleft%20%5C%7B1%2C...%2Ck%20%5Cright%20%5C%7D)  
    
    ![](https://latex.codecogs.com/gif.latex?%5Csum_%7Br%7Dp%28r%7Ca%29r)  
    
  - 목표: Maximize the expected reward  
  
    ![](https://latex.codecogs.com/gif.latex?argmax_a%20q_*%28a%29)  
    
  - 위의 의사 예에서 다양한 확률분포 사용 (베르누이, 이항, 균등 분포)  
  - Bandits ⊂ RL  
  
##### 요약  
- 불확실성 아래에서의 의사 결정은 K-armed bandit 문제로 볼 수 있다.  
- 사용되는 기본 아이디어들: actions, rewards, value function  

---  

## 1.3 What to Learn? Estimating Action Values  

### 1.3.1 Learning Action values    
- `Value of an Action`  
  - = `expected reward` when that action is taken  
  
    ![](https://latex.codecogs.com/gif.latex?q_*%28a%29%20%5Cdoteq%20E%28R_t%7CA_t%3Da%29)  
    
  - ![](https://latex.codecogs.com/gif.latex?q_*%28a%29): unknown → 추정(`estimate`)해야  
  
    (value of each action을 안다면, 항상 highest value를 지니는 액션을 택하면 되기에 k-armed bandit   
    문제를 해결하는 건 식은 죽 먹기 이지만, 대충 추정은 하고 있다고 할지라도, 확실한 action value를  
    알지는 못한다고 가정)  
    
    → ![](https://latex.codecogs.com/gif.latex?Q_t%28a%29): the estimated value of action a at time step t  
    
      - 물론 ![](https://latex.codecogs.com/gif.latex?Q_t%28a%29)가 ![](https://latex.codecogs.com/gif.latex?q_*%28a%29)에 가깝기를 바랄 것  
      
      `결론적으로, Q는 q의 estimator`  
    
  
  - `Sample-Average Method`  
  
    - ![](https://latex.codecogs.com/gif.latex?q_*%28a%29)를 추정하는 방법 중 하나  
    
    - ![](https://latex.codecogs.com/gif.latex?Q_*%28a%29%20%5Cdoteq%20%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7Bt-1%7DR_i%7D%7Bt-1%7D)  
    
      = sum of rewards when a taken prior to t / number of times a taken prior to t  
      
      = ![](https://latex.codecogs.com/gif.latex?%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7Bt-1%7DR_i%5Ccdot%20%5Cmathbb%7BI%7D_%7BA_i%3Da%7D%7D%7B%5Csum_%7Bi%3D1%7D%5E%7Bt-1%7D%5Cmathbb%7BI%7D_%7BA_i%3Da%7D%7D)  
      
    - t-1을 쓴 건 time t에서의 value는 t 이전의 action들에 근거한 것이기 때문  
    - 따라서 어떤 action이 아직 일어나지 않았다면, value = 0처럼 default value를 줄 것  
    - 'Of course this is just one way to estimate action values, and not necessarily the best one'  
    
    
  ```  
  다시 예로 돌아가서, 약이 말을 잘 들으면 reward로 1 잘 안들으면 0
  ```  
  
- `Greedy Action`  
  - 현재 가장 큰 추정값을 가진 action 
  - Greedy Action을 선택한다는 것 = 현재 가진 정보를 이용한다는 것  

- `Non-greedy action`  
  - 한편, agent는 현재의 reward 정보들을 무시하고 다른 action들에 대한 
    더 많은 정보들을 얻으려는 목적에서 non-greedy action 사용 가능  
 
`Exploitation is the right thing to do to maximize the expected reward on the one step, 
but exploration may produce the greater total reward in the long run.`  
 
      
  ```  
  요컨대  
  Greedy action → 'exploit' current knowledge of the values of the actions  
  Nongreedy action → 'explore'
  ```
      
- `Exploration-exploitation dilemma`  
  - The agent cannot choose to both explore and exploit at the same time. 
  - `강화학습의 근본적인 문제중 하나`  

### 1.3.2 Estimating Action Values Incrementally  
- Situation:  

  ```  
  - 하루에 수 백만 방문자를 기록하는 웹사이트의 운영자
  - 이 문제를 k-armed bandit 문제로 볼 수 있음  
  - 광고 실어서 어떻게 가장 큰 수익을 얻을 수 있을까?  
  - 몇 백만 클릭을 데이터를 저장하지 않고 어떻게 추정할 수 있을까? 
    → 이 소주제  
  ```  
  
- Sample-average 방법  
  - 재귀적으로 쓰일 수 있음  
    → 모든 이전 데이터들을 저장해야만 하는 상황을 피할 수 있음  
  - Q → q by `LLN`   
    
  - `Incremental update rule`  
    ![](https://latex.codecogs.com/gif.latex?Q_%7Bn&plus;1%7D%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5EnR_i)  
    ![](https://latex.codecogs.com/gif.latex?%3D%20%5Cfrac%7B1%7D%7Bn%7D%28R_n%20&plus;%20%5Csum_%7Bi%3D1%7D%5E%7Bn-1%7DR_i%29)  
    ![](https://latex.codecogs.com/gif.latex?%3D%20%5Cfrac%7B1%7D%7Bn%7D%28R_n%20&plus;%20%28n-1%29%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi%3D1%7D%5E%7Bn-1%7DR_i%29%29)  
    ![](https://latex.codecogs.com/gif.latex?%3D%20%5Cfrac%7B1%7D%7Bn%7D%28R_n%20&plus;%20%28n-1%29Q_n%29)  
    ![](https://latex.codecogs.com/gif.latex?%3D%20%5Cfrac%7B1%7D%7Bn%7D%28R_n%20&plus;%20nQ_n%20-%20Q_n%29)  
    ![](https://latex.codecogs.com/gif.latex?%5Ctherefore%20Q_%7Bn&plus;1%7D%20%3D%20Q_n%20&plus;%20%5Cfrac%7B1%7D%7Bn%7D%28R_n-Q_n%29)  
    
  - ![](https://latex.codecogs.com/gif.latex?%5Ctherefore%20Q_%7Bn&plus;1%7D%20%3D%20Q_n%20&plus;%20%5Cfrac%7B1%7D%7Bn%7D%28R_n-Q_n%29)  
  
  : 이 식은 지겹도록 보게 될 식  
  : ![](https://latex.codecogs.com/gif.latex?R_n): current reward  
  : `New Estimate ← OldEstimate + Stepsize(Target - Old Estimate)`  
  
  ```
  - Error: [Target - OldEstimate]  
  - Error ↓ as taking a step toward the 'Target'   
  - Target: nth reward  
  - Stepsize: changes from time step to time step  
  - The size of the step is determined by our step size parameter and the error of our old estimate. 
  
  → 이 식: a general rule for updating the estimate incrementally. 
  - step size: 0~1 사이의 값을 반환하는 n의 함수
  ```  
  - ★ 하지만, sample-average method: `stationary bandit problem`에 적절  
    (= reward possibility가 시간에 상관없이 일정한 경우)  
  
- `Non-stationary bandit problem`  
  - Reward의 분포가 시간에 따라 변함  
    → 과거의 reward보다 최근의 reward에 더 많은 weight를 주어야 할 것  
    → 가장 인기있는 방법 중 하나: stepsize parameter를 `상수`처리  
  - 의사는 이 변화를 알 수는 없지만 그 변화를 감지하고자 할 것  
    → 방법1: 고정된 step size사용  
      - 가장 최근의 Reward가 그 이전의 reward들 보다 추정량에 더 큰 영향을 미칠 것  
    ![](http://drive.google.com/uc?export=view&id=18xc6QOB8KPgRvoyfvhPXwWQ5ibvgpNqy)  
    - Weighting fades exponentially with time  
    - x축의 오른쪽로 갈수록, 더 오래전으로 돌아가는 것  
    
  - `Decaying past rewards`  
    ![](https://latex.codecogs.com/gif.latex?Q_%7Bn&plus;1%7D%20%5Cdoteq%20Q_n%20&plus;%20%5Calpha%5BR_n-Q_n%5D)  
    ![](https://latex.codecogs.com/gif.latex?Q_%7Bn&plus;1%7D%20%3D%20%5Calpha%20R_n%20&plus;%20%281-%5Calpha%29Q_n)  
    (![](https://latex.codecogs.com/gif.latex?%5Calpha%20%5Cin%20%280%2C1%5D))  
    - → Recursive form  ...  
    ![](https://latex.codecogs.com/gif.latex?%3D%5Calpha%20R_n%20&plus;%20%281-%5Calpha%29%5Calpha%20R_%7Bn-1%7D%20&plus;%20%5Ccdots%20%281-%5Calpha%29%5E%7Bn-1%7D%5Calpha%20R_1%20&plus;%20%281-%5Calpha%29%5EnQ_1)  
    ![](https://latex.codecogs.com/gif.latex?%3D%20%281-%5Calpha%29%5EnQ_1%20&plus;%20%5Csum_%7Bi%3D1%7D%5En%7B%5Calpha%281-%5Calpha%29%5E%7Bn-i%7DR_i%7D)  
    ![](https://latex.codecogs.com/gif.latex?Q_1) ← initial action-value  
    
    ```  
    첫번째항: Q1이 시간의 흐름에 따라 exponential하게 줄어든다.  
    두번째항: Reward가, 시간이 현재에서 멀어질수록 그 합에 exponential하게 덜 기여  
    → 종합해보자면, 
      초기값의 영향력은 데이터가 많아질수록 0으로 가까이가고, 
      가장 최근의 Reward가 현재 상태에 가장 큰 영향을 미친다. 
    ```  
    
  - ![](https://latex.codecogs.com/gif.latex?Q_1) ← initial action-value  
  
#### 요약  
- incremental sample-average method  
- generalized the incremental update rule into a more general update rule  
- A constant stepsize parameter can be used to solve a non-stationary bandit problem.  

---  

## 1.4 Exploration vs Exploitation Tradeoff  

- cf) [multi-armed bandit pb.와 algorithm에 대한 글](https://towardsdatascience.com/solving-multiarmed-bandits-a-comparison-of-epsilon-greedy-and-thompson-sampling-d97167ca9a50)  
- cf) [multi-armed bandit pb.와 algorithm에 대한 글2](https://brunch.co.kr/@chris-song/62)  

### 1.4.1 What is the trade-off?    
- 언제 Explore하고 싶어하고, 언제 Exploit하고 싶어할까? 라는 물음  
  - Exploration  
    - Agent로 하여금 액션에 대한 지식을 향상시키도록 해줌  
    - lead to 'long-term benefit'  
  - Exploitation  
    - Agent의 현재의 estimated values를 사용(exploit)  
    - 가장 큰 reward를 얻으려 greedy action을 선택  
      (`Greedy` action selection always exploits current knowledge to maximize immediate reward;  
      it spends no time at all sampling apparently inferior actions to see if they might really  
      be better)  
    - 하지만 estimated values에 대해 greedy해진다는 것은 가장 큰 reward를 얻지 못할 수도 있다는 것  
      → `sub-optimal`한 행동을 하게만들 수도 있음  
    
  ■ Exploration-Exploitation Dilemma ■  
  
- 그렇다면, 다시 원래의 물음으로 돌아가서, 'How do we choose when to explore and when to exploit?`  
  - Explore → more accurate estimates of values  
  - Exploit → more reward  
  - 둘을 동시에 달성하기: 불가  
  
#### Epsilon-Greedy Action Selection  
- 위 문제를 해결하기 위한 방법 중 하나: 
  - `Choose Randomly`  
  - 대부분의 경우 Exploit, 적은 확률로 Exploring  
    (Behave greedily most of the time, but every once in a while, say with small probability ε, instead select randomly from among all the actions with equal probability, independently of the action-value estimates)  
    ```  
    ex) 
    주사위 눈1 → Explore  
    2,3,4,5,6 → greedy  
    ```  
  - `Epsilon`:  explore을 선택할 확률  (이 경우 1/6)  
   
  #### Epsilon-Greedy를 다음과 같이 써 볼 수 있다.  
  ![](https://latex.codecogs.com/gif.latex?A_t%20%5Cleftarrow%20%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D%20argmax_a%20Q_t%28a%29%20%26%20w.p%20%5C1-%5Cvarepsilon%20%5C%5C%20a%20%5Csim%20Uniform%28%5Cleft%20%5C%7B%20a_1%2C%20...%2C%20a_k%20%5Cright%20%5C%7D%5D%29%20%26%20w.p%20%5C%20%5Cvarepsilon%20%5Cend%7Bmatrix%7D%5Cright.)  
  
- 10-armed Testbed   
  - 10 actions  
  - x축: action 1-10, y축: action별 Reward distribution  
  
    ```  
    The true value q*(a) of each of the ten actions were selected according to a normal dist'n
    with mean zero and unit variance, and then the actual rewards were according to a mean q*(a)
    unit variance normal dist'n, as suggested by the gray dist'ns below.  
    ```  
    
    ![](http://drive.google.com/uc?export=view&id=1tU6g6oTqRqHw0XE2NHF7DmLKi1LyDwsx)  
    ![](http://drive.google.com/uc?export=view&id=1_eFy-IjUQFfOfOkvq30zl9KEzHAO9A_v)  
    (이런 식으로 10개의 각 액션에 대하여 normal distribution을 따르는 reward(q star)를 각 step마다 랜덤하게 추출)  
    
    ![](https://latex.codecogs.com/gif.latex?q_*%281%29%2C%20...%2C%20q_*%2810%29)    
    
    - ![](https://latex.codecogs.com/gif.latex?q_*%28a%29%20%5C%3B%20selected%20%5C%3B%20from%20%5C%3B%20N%280%2C1%29)  
    - ![](https://latex.codecogs.com/gif.latex?R_t%20%5C%3B%20selected%20%5C%3B%20from%20%5C%3B%20N%28q_*%28A_t%29%2C1%29)  
  
    - 각 step마다 1-10개의 action들 중에 분포를 따르는 하나의 q star 값 추출.  
    ![](http://drive.google.com/uc?export=view&id=1g9K31yj_NDBVT_kVqFNPVxDJpcayRStv)  
    - 커브를 그려보면 다음과 같음  
    ![](http://drive.google.com/uc?export=view&id=1ln5687YjxZ0fy9rugUbTRfxz71Mv48VY)  
    - 이만큼의 큰 noise로는 어떠한 결론도 얻어내기 어려움  
    
    - 이 과정(Run)을 여러번 반복해서 평균을 내서 플라팅하면 보다 안정적인 그래프가 도출  
    - 이 그래프도 시행횟수(Run)이 많아질수록, 그 평균치는 훨씬 더 안정된 그래프가 도출  
    ![](http://drive.google.com/uc?export=view&id=1m6J9sHUHohNpgJXZIP32oykFZILMvEIG)  
    
    - 입실론 값의 변화에 따른 average reward 값 추세의 변화 
    ![](http://drive.google.com/uc?export=view&id=1rYS793iBC4rVi7vI6yzTY1qCKyj30Fum)  
      - 예를 들어 입실론이 0.01이면 평균 리워드값이 시간이 지날수록 계속 상승  
      - 입실론이 0.1이면 다른 방법들보다 더 이른 시간에 더많은 reward. 하지만 300 step이후엔 평평  
      - greedy 방법 → 장기적으로는 bad 왜냐하면 suboptimal action에 빠져있었기 때문  

#### 요약  
- tradeoff between exploration and exploitation  
- introduced epsilon-greedy which is a simple method for balancing exploration and exploitation  

### 1.4.2 Optimistic Initial Values  
- Uncertainty에 대해 긍정적인 입장을 취하는 것  
  → exploration-exploitation 문제에서 매우 일반적인 전략 중 하나  
 
- 의사 예시  

  ```  
  - 의사가 A, B, C 약 모두가 100% 효과있다는 가정을 갖고 있는 사람  
  - 그렇다면 의사는 이 세 약을 랜덤하게 환자들한테 처방해줄 것  
  - 단, 예를 들어, C라는 약이 환자들에게 효과가 없다는 사실을 발견했다면, 
    이제 의사는 A,B만 처방할 것  
  - 의사는 남은 A,B 중 어느 약이 환자들에게 효과가 없다는 사실이 발견되기 전까지는 
    100% 효과있다는 자신의 가정을 계속 유지하면서 계속 A,B를 랜덤하게 처방할 것  
  ```  
  
  - 이전에는 initial estimated values(![](https://latex.codecogs.com/gif.latex?Q_1%28A%29%3D0%2C%20Q_1%28B%29%3D0%2C%20Q_1%28C%29%3D0))를 0으로 줬었는데, 
    이번에는 좀 optimistic하게 A,B,C 모든 약에 대해 2라는 initial estimated values를 줘  
    (observed reward가 optimistic initial estimate보다 작다는 점에 유의)  
  
  ![](https://latex.codecogs.com/gif.latex?Q_%7Bn&plus;1%7D%20%5Cleftarrow%20Q_n%20&plus;%20%5Calpha%28R_n-Q_n%29)  
  & Let α = .5  
  
  - Initial(상황 가정)  
  
  | |A|B|C|  
  |-|-|-|-|  
  |Q1|Q1(A)=2.0|Q1(B)=2.0|Q1(C)=2.0|  
  |q*|q*(A)=0.25|q*(B)=0.75|q*(C)=0.5|  
  
  - 현재, 약 A,B,C 모두에 대해 Q는 모두 같아. 따라서 의사는 약 A,B,C 중 랜덤하게 선택  
  - A를 선택했고, 환자는 '저 나아졌어요~'라고 했다고 가정   
  
  | |A|B|C|  
  |-|-|-|-|  
  |Q2|Q2(A)=1.5|Q2(B)=2.0|Q2(C)=2.0|  
  |q*|q*(A)=0.25|q*(B)=0.75|q*(C)=0.5|  
  
  - 이제 의사는 highest-estimated value 사이에서 골라야 하기 때문에 B,C 중 랜덤하게 선택  
  - B를 선택했고, 환자는 '저 안나아졌어요...'라고 했다고 가정  
  
  | |A|B|C|  
  |-|-|-|-|  
  |Q3|Q3(A)=1.5|Q3(B)=1.0|Q3(C)=2.0|  
  |q*|q*(A)=0.25|q*(B)=0.75|q*(C)=0.5|  
  
  - 이제 세 번째 환자에 대해선 C를 처방할 것  
  - 환자는 '저 나아졌어요!'라고 했다고 가정  
  
  | |A|B|C|  
  |-|-|-|-|  
  |Q4|Q4(A)=1.5|Q4(B)=1.0|Q4(C)=1.5|  
  |q*|q*(A)=0.25|q*(B)=0.75|q*(C)=0.5|  
  
  - 이 과정을 계속 반복  
  - `optimistic initial values가 학습의 초기에 exploration을 일어나도록 하고 있다.`  
  - 이 경우에는 초반 3번에 모든 약을 다 처방함  
  
  - 10-armed Bandit 예에서도 적용 가능  
    ![](http://drive.google.com/uc?export=view&id=1wuMCoJcforNX4GqT6NA2h35OuWUYjaol)  
    - 초기에는 optimistic agent가 더 많이 explore하기 때문에 성능 더 안 좋음  
    - exploration은 시간이 지날수록 줄어듦.  
    - (encouraging exploration optimistic initial values)  
    - quite effective on `stationary` problem    
    
- 하지만 Optimistic Initial Values 사용하는 것이 반드시 exploration and exploitation문제 해결을 위한 최선의 방법인 것은 아님  

  - `한계`  
  
      
    1. Optimistic initial values는 exploration을 초기에만.  
      - 시간이 좀 지난 후에는 exploration 안 하기 시작  
    2. non-stationary problem에는 적합하지 않음  
      - action value가 시간이 좀 지난 후에는 바뀔 수도 있는데, 
        optimistic agent는 이미 특정 액션에 지나치게 고착화되어버려서, 
        현재에는 다른 액션이 낫다는 사실을 알아차리지 못할 것  
      - (어떤 방법이든 initial condition에 집중하는 방법은 일반적인 
        nonstationary case에 도움이 별로 안됨 → 사실 sample-avg에도 마찬가지) 
    3. optimistic initial value를 어떻게 설정해야할지 모르는 경우도 많음  
      - 실제로는 최대 reward 값이 얼마인지를 모를 수도 있기 때문에  
     
   
  - 그럼에도 불구하고 Optimistic initial values는 매우 유용한 heuristic  
      
#### 요약  
- Optimistic initial values encourage early exploration  
- Described limitations of optimistic initial values  

### 1.4.3 Upper-Confidence Bound(UCB) Action Selection  
- (cf) 어떤 글에서, Multi-Armed Bandit 문제에서는 UCB를 베이스로 한 알고리즘이 제일 각광받고 있다고는 하는데...?)  

- `Q(a)`(Our current estimate for action A)에 대한 confidence interval  
- lower bound & upper bound  
- C.I가 좁다: value of action A가 estimated value 가까이에 있다.  
- C.I가 넓다: value of action A가 estimated value 가까이에 있는지에 대해 uncertain하다.  

- if uncertain about sth → should optimistically assume that it is good.  
- C.I가 얼마나 넓은가에 상관없이 Upper bound가 가장 큰 Action을 선택  

![](https://latex.codecogs.com/gif.latex?A_t%20%5Cdoteq%20argmax%5BQ_t%28a%29%20&plus;%20c%5Csqrt%7B%5Cfrac%7Bln%20t%7D%7BN_t%28a%29%7D%7D%5D)   
    - ![](https://latex.codecogs.com/gif.latex?lnt): natural logarithm of t  
    - ![](https://latex.codecogs.com/gif.latex?N_t%28a%29): 시간 t 이전에 action a가 총 몇 번 선택되었는가  
    - c: exploration 정도를 컨트롤  
- select `highest estimated value + our upper-confidence bound exploration term`  
- upper-bound term은 세 부분으로 나누어짐  

  
- C: user-specified parameter
  - controls the amount of exploration 
   
  
  ![](http://drive.google.com/uc?export=view&id=1cg4tfOyXcK1hKlvwP3MsQ30-_0QSC_q-)  
  ![](http://drive.google.com/uc?export=view&id=1XLG3WFEPMJj7iGOXPeMlAg7hc2jTDJfJ)  
  
    - a가 선택될 때마다, 불확실성은 줄어듦.  (![](https://latex.codecogs.com/gif.latex?N_t%28a%29)↑)  
    - a가 아닌 다른 액션이 선택될 때마다 불확실성은 늘어남  (t ↑, ![](https://latex.codecogs.com/gif.latex?N_t%28a%29)는 그대로)  
    - 자연로그를 썼기에 증가분은 시간이 갈수록 줄어든다. 하지만, `unbounded`  
      → 모든action들은 결국에는 선택될 것  
      → 하지만 actions with lower value estimates나 actions that have already been selected frequently들은 시간이 지날수록 선택되는 빈도가 줄어들 것  
  
  - 역시 10-armed bandit 문제에서 2000번의 독립시행   
  ![](http://drive.google.com/uc?export=view&id=1pp5t7nawh4cF8VsOoNFYNV-ejPurpL82)  
  - 100 steps 이후에는 UCB가 Epsilon-greedy보다 평균적으로 더 큰 reward를 얻고 있음  
  - 초기에, UCB는 systematic하게 uncertainty를 줄이려고 더 많이 explore  
  - UCB의 exploration은 시간이 갈수록 줄고 있는 반면, Epsilon-greedy는 시간 마다 10%의 확률로 explore  
  
#### 요약  
- Upper-Confidence Bound action-selection uses uncertainty in the value estimates for balancing exploration and exploitation  

### 1.4.4 Jonathan Langford: Contextual Bandits for Real World Reinforcement Learning  
- 강화학습
  : works with simulator  
  - A simulator provides observations, and then a learning algorithm has a policy which chooses an action  
  - The simulator then processess and returns in reward  
  
  - 하지만, 현실에서는  
  
    ```  
    'observation이 simulator로부터 오는가?' → 아니
    'observation에 근거한 action이 같은가? → 아니 (observation의 성격부터가 다르다)  
    └ same policy라도 다른 결과를 낳는다. 
    '실세계에서 얻는 reward와 observation으로부터 얻는 reward가 같은가?' → 아니
    ```  
    
    `결론적으로 시뮬레이터로부터 학습을 하더라도 이걸 현실에 적용하기는 많은 경우에 마땅찮다. 불가능할 수도`  
- `Q: 그렇다면 Real-World Reinforcement Learning은 어떻게 하지?`  
  `A: A shift in priorities`

|↓|↑|  
|-|-|  
|Temporal Credit Assignment|Generalization|  
|Control environment|Environment controls|  
|Computational efficiency|Statistical efficiency|  
|State|Features|  
|Learning|Evaluation|  
|Last policy|Every policy|  

#### Contextual Bandits?  
- Repeatedly  

  ```  
  1. Observe features x  
  ex. geolocation of users etc. 
  2. Choose action a ∈ A
  3. Observe reward r  
  ```  
  
  - key change: no credit assignment problem  
  
|Year|Algorithm|  
|----|---------|  
|1995|EXP4 paper|
|2007|Epoch Greedy|  
|2010|Personalized News|  
|2011|Comp/Stat opt algo|  
|2014|Better Algo|  
|2016|Created first version of 'Decision Service'|  
|2019|First RL Service Product 'Azure Cognitive Services Personalizer|  
|2019|AI system of the year|  

Relevant stuff  
1) Contextual Bandit Tutorial: [http://hunch.net/~rwill](http://hunch.net/~rwill)  
2) Personalizer Service: [http://aka.ms/personalizer](http://aka.ms/personalizer)  
3) Vowpal Wabbit CB algs: [http://vowpalwabbit.org](http://vowpalwabbit.org) 

---  

# 1주차 전체 요약  
- `Agent` taking `Actions` and receive `Rewards` based on the action selected  
- value of each action : expected reward received, when taking that action  
- Value function(![](https://latex.codecogs.com/gif.latex?q_*%28a%29)) is unknown to agent  
  → Then introduced `Sample-Average Method`(![](https://latex.codecogs.com/gif.latex?Q_t%28a%29)) for estimating ![](https://latex.codecogs.com/gif.latex?q_*%28a%29)  
- Incremental update rule  
  - Recursive하기 때문에 
    ![](https://latex.codecogs.com/gif.latex?N_t%28a%29) # The number of times each action has been taken  
    ![](https://latex.codecogs.com/gif.latex?Q_t%28a%29) # The previous estimate  
    이것만 필요  
    - Stepsize를 더 generic하게 상수 stepsize를 써서, 시간에 따라 변하는 Bandit 문제를 더 효율적으로 해결할 수 있었다.  

- `Exploration vs Exploitation`  
  - Exploration: improve knowledge for long-term benefit  
  - Exploitation: exploit knowledge for short-term benefit  

- How to choose when to explore and when to exploit?  
  - (0. Greedy Method)
  - 1. Epsilon-Greedy Action Selection  
  - 2. Optimistic Initial Values  
  - 3. UCB Action Selection  
  `이 모든 게 exploration과 exploitation 사이에서 균형을 잡으려는 노력`  

  - 강의에서는 Gradient Bandit Algorithm 언급x)

- 이게 전부 Reinforcement Learning의 기본 컨셉  
- Bandit Pb ⊂ RL

 
