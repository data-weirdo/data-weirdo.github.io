---
layout: post
title:  "Fundamentals of Reinforcement Learning Week4"
subtitle:  "FRL_Week4"
categories: ml
tags: rl
comments: true
---

Policy Evaluation과 Improvement 및 Dynamic Programing, (General) Policy Iteration에 대한 글입니다.  

---  

## 4.1 Policy Evaluation  
- Recall  
  ![](https://latex.codecogs.com/gif.latex?v_*%28s%29%20%3D%20%5Cunderset%7Ba%7D%7Bmax%7DE%5BR_%7Bt&plus;1%7D&plus;%5Cgamma%20v_*%28S_%7Bt&plus;1%7D%29%7CS_t%3Ds%2C%20A_t%3Da%5D)  
  ![](https://latex.codecogs.com/gif.latex?%3D%20%5Cunderset%7Ba%7D%7Bmax%7D%5Csum_%7Bs%2Cr%7Dp%28s%27r%7Cs%2Ca%29%5Br&plus;%5Cgamma%20v_*%28s%27%29%5D)  
  
  ![](https://latex.codecogs.com/gif.latex?q_*%28s%2Ca%29%20%3D%20E%5BR_%7Bt&plus;1%7D&plus;%5Cgamma%20%5Cunderset%7Ba%27%7D%7Bmax%7Dq_*%28S_%7Bt&plus;1%7D%2C%20a%27%29%7CS_t%3Ds%2C%20A_t%3Da%5D)  
  ![](https://latex.codecogs.com/gif.latex?%5Csum_%7Bs%27%2Cr%7Dp%28s%27%2Cr%7Cs%2Ca%29%5Br&plus;%5Cgamma%20%5Cunderset%7Ba%27%7D%7Bmax%7Dq_*%28s%27%2Ca%27%29%5D)  
  
  ```  
  DP algorithm은 위와 같은 Bellman Equation을 원하는 value function의 근사를 향상시키기 위한 update rule로 바꿈으로써 얻어짐.
  ```  
  
### 4.1.1 Policy Evaluation vs. Control  
- 두 가지 별개의 task: `policy evaluation` & `control`  

- `Policy Evaluation (Prediction)`  
  : 특정 policy에 대해 value function을 결정하는 일(Task of determining the value function for a specific policy.)  
  
    ```  
    We consider how to compute the state-value function v_π for an arbitrary policy π.  
    This is called policy evaluation in the DP literature.  
    ```  
    
- `Control`  
  : 강화학습의 궁극적 목적(ultimate goal of RL)  
  - Policy Evaluation은 보통, Control을 위해 필요한 첫 단계  
    ∵ Policy가 얼마나 좋은지를 평가할 만한 지표가 없다면 policy를 개선한다는 것이 어렵기 때문에.  
      
- `Dynamic Programming`  

  ```  
  - DP: MDP(Markov Decision Process)와 같은 environment의 완벽한 모델이 주어졌을 때 optimal policies를 계산하는 데에 사용될 수 있는 알고리즘의 집합체 (DP refers to a collection of algorithms that can be used to compute policies given a perfect model of the environment as a MDP) 
  - Classical DP: 완벽한 모델에 대한 가정과, 엄청난 연산 비용 때문에 이용에 어느정도 한계가 있음.  
  - 하지만, 다양한 다른 시도들도 DP와 동일한 효과를 얻기 위한 시도로 볼 수 있음. (적은 연산과, environment에 대한 완벽한 가정 없이도)
  ```  
  
  - `policy evaluation`과 `control` 문제를 동시에 해결  
  - 이 둘 모두에 대하여 iterative한 알고리즘을 정의하기 위해, Dynamic Programming 알고리즘은 `Bellman Equation`을 사용  
  - 우선, policy evaluatoin과 control에 대해 명확하게 정의부터 하고 들어가겠음.  
  
- Policy evaluation  
  - 특정 policy π에 대해, state-value function ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D)을 결정하는 Task.   
    - Recall  
      ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D%28s%29%20%5Cdoteq%20E_%7B%5Cpi%7D%5BG_t%7CS_t%3Ds%5D)  
      - The value of a state under a policy π is the expected return from the state if we act according to π  
      - The return itself: discounted sum of future rewards  
    - Recall2  
      - Bellman Equation: ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D) 찾는 문제를 단순히 연립 방정식의 문제로 바꾸어 놓았다.  
        → `Policy evaluation의 문제도 결국 연립방정식 해결의 문제`  
      - 원론적으로, 이 문제를 다양한 선형 대수의 방법에서 접근 가능  
      - `In practice, the iterative solution methods of dynamic programming are more suitable for general MDPs.`  
        ![](http://drive.google.com/uc?export=view&id=1BebiZrEIHBOez8PV6ve1NkDuMYB8M4p2)  
        
      ```  
      The existence and uniqueness of v_π are guaranteed as long as either γ < 1 
      or eventual termination is guaranteed from all states under the policy π
      ```  
     
- Control  
  - Policy를 향상시키는 Task (task of improving a policy)  
    - Recall  
    - ![](https://latex.codecogs.com/gif.latex?%5Cpi_2) is good as or better than ![](https://latex.codecogs.com/gif.latex?%5Cpi_1)?  
      - Value under ![](https://latex.codecogs.com/gif.latex?%5Cpi_2) is greater than ![](https://latex.codecogs.com/gif.latex?%5Cpi_1)   
    - ![](https://latex.codecogs.com/gif.latex?%5Cpi_2)is `strictly` better than ![](https://latex.codecogs.com/gif.latex?%5Cpi_1)?  
      - Value under ![](https://latex.codecogs.com/gif.latex?%5Cpi_2) is `strictly` greater than ![](https://latex.codecogs.com/gif.latex?%5Cpi_1)      
  
  - Control Task의 목적  
    - `to modify a policy to produce a new one which is strictly better`  
    - can try to improve the policy repeatedly to obtain a sequence of better and better policies.  
      - 이 일이 더이상 불가능해지면, 현재 policy보다 strictly better인 policy가 없는 것.  
      - `So, the current policy must be equal to an optimal policy`  
        = `Control task complete.`  
        
![](http://drive.google.com/uc?export=view&id=1aeAhx8VvgJwIOSKEIV9vWi61FClpjGim)    
- Enviornment `p`에 대한 접근이 가능하다고 가정  
  -  접근이 가능하다고 하더라도, `value function`과 `optimal policy`를 계산하기 위한 생각과 알고리즘이 필요  
  
- Dynamic Programming은 다양한 Bellman equation (앞에서 봐왔던) & environment `p`에 대한 지식을 함께 사용   
  (고전적인 DP는 ENVORNMENT와의 상호작용은 생각하지 않음. 대신 DP방법을, 주어진 MDP Model에 대해 value function과 optimal policies를 계산해내는 데에 사용)   
  `Dynamic programming uses the various Bellman equations we've seen, along with knowledge of p, to work`out value functions and optimal policies.`  
  - DP: 강화학습의 알고리즘들 이해하는 데에 유용  
  - 대부분의 RL이 모델이 없는 DP에의 approximation으로 볼 수 있음  
    (Most RL algorithms can be seen as an approximation to dynamic programming without the model.)  
    
  ```  
  Dynamic Programming algorithm provides methods for solving two tasks: 'Policy Evaluation' & 'Control',
  as long as we have direct access to environmential dynamics (P(s',r|s,a)).
  In the RL problem, we will not assume we know the dynamics. AFter all, in the real world, 
  we can't always expect to know the effect of each of our actions until we try them. 
  Dynamic programming algorithms provide an essential foundation for the reinforced learning algorithms, we   will cover in the rest of the specialization.
  ```   
    
- Summary  

  ```  
  - Policy evaluation is the task of determining the state-value function v_π, for a particular policy π. 
  - Control is the task of improving an existing policy.  
  - Dynamic programming techniques can be used to solve both these tasks, if we have access to the dynamics 
    function p  
  ```  
  
### 4.1.2 Iterative Policy Evaluation  
- Dynamic programming algo.  

  ```  
  - Dynamic Programming  
    - The term dynamic programming (DP) refers to a collection of algorithms that 
      can be used to compute optimal policies given a perfect model of the environment 
      as a Markov decision process (MDP)
  ```  
  
  - 결국, Bellman equation을 update rule로 돌림으로써 얻어짐  
  - 그 중 하나: `Iterative policy evaluation`  
  
- 이 강의에서 배울 것  

  ```  
  - Outline the iterative policy evaluation algorithm for estimating state values under a given policy.  
  - Apply iterative policy evaluation to compute value functions  
  ```  
  
  - Recall  
    - Bellman equation: gives recursive expression for ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D)  
      ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D%28s%29%20%3D%20%5Csum_a%5Cpi%28a%7Cs%29%5Csum_%7Bs%27%7D%5Csum_%7Br%7Dp%28s%27%2Cr%7Cs%2Ca%29%5Br%20&plus;%20%5Cgamma%20v_%7B%5Cpi%7D%28s%27%29%5D)  
      ↓  
  
- Iterative Poilcy Evaluation  
  - Take the Bellman equation & directly use it as an update rule  ← 넘나 단순  
    ![](https://latex.codecogs.com/gif.latex?v_%7Bk&plus;1%7D%28s%29%20%5Cleftarrow%20%5Csum_a%5Cpi%28a%7Cs%29%5Csum_%7Bs%27%7D%5Csum_%7Br%7Dp%28s%27%2Cr%7Cs%2Ca%29%5Br%20&plus;%20%5Cgamma%20v_%7Bk%7D%28s%27%29%5D)  
    - value function에 계속해서 조금씩 더 나은 근사의 연속이 될 것.  
      (This will produce a sequence of better and better approximations to the value function.)  
      
    - arbitrary initialization for approximate value function ![](https://latex.codecogs.com/gif.latex?v_0)  
    - ![](https://latex.codecogs.com/gif.latex?v_0) 부터 ![](https://latex.codecogs.com/gif.latex?v_1), ![](https://latex.codecogs.com/gif.latex?v_2) 계속 업데이트  
      (Each iteration applies this updates to every state S) : `Sweep`  
      
      ```  
      - Each iteration of iterative policy evaluation updates the value of every state once to produce  
        the new approximate value function
      ```  
      
    - 매 iteration마다 더 나은 approximation을 낳음  
    - 이 update가 변동이 없어지면, (If ![](https://latex.codecogs.com/gif.latex?v_%7Bk&plus;1%7D) = ![](https://latex.codecogs.com/gif.latex?v_%7Bk%7D)) : We have found the value function.  
      ∵ ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D): the unique solution of Bellman Equation.  
        - v가 더이상 업데이트 되지 않는다는 말은, ![](https://latex.codecogs.com/gif.latex?v_%7Bk%7D)가 이미 Bellman Equation을 따른다는 말.  
        - 극한의 관점에서, v는 반드시 수렴하게 되어 있음.  
        
    - Iterative Policy Evaluation을 계산?,   
      : Store 2 arrays (Each has 1 entry for every state.)  
      - Array 1 (`V`)  
        - stores the current approximate value function  
      - Array 2 (`V'`)  
        - stores the updated values  
      ![](http://drive.google.com/uc?export=view&id=1c6IL8wkXXLsQkHyL06YB4z0Es9k-ZPd7)    
        : By using two arrays, we can compute the new values from the old one state  
          at a time without the old values being changed in the process.  
      - Full sweep 끝에, 모든 새로운 값들을 V에다가 쓸 수 있음.     
      ![](http://drive.google.com/uc?export=view&id=1Yy6388oU_bsgJiahLwXKaIzcWwDVz2pz)  
      
      cf) 이렇게 두 개의 array 쓰는 것 말고, 하나의 array를 쓰는 것도 가능함. (변경값 바로바로 덮어쓰기)   
      - 수렴 보장. 오히려 속도는 빨라질 것  
     
     - Iterative Policy Evaluation 예   
     ![](http://drive.google.com/uc?export=view&id=15Nvh3frvkUK4nBzqCE5pvx2QmbBpq6im)  
      - episodic MDP → γ = 1 (undiscounted)  
      - each action: deterministic  
      - terminal state: gray ones  
      - uniform random policy (0.25)  
      ![](http://drive.google.com/uc?export=view&id=1lRssU3Wpt4ldsPLyfOo6mTOJXLCgXgnt)  
      ![](http://drive.google.com/uc?export=view&id=11nwtnqvgOnelW48dBgyaH-gXDNgizv5g)  
      ...
      ![](http://drive.google.com/uc?export=view&id=1ungSaEYG9v-Byia9z1mPsJWv9YfdL0lm)  
      ![](http://drive.google.com/uc?export=view&id=1RFExKB1bd8inoOJKe4Z30j2iP248t9E4)  
      - 이까지가 1 sweep !!  
      - 이를 반복~  
      
      ![](http://drive.google.com/uc?export=view&id=1geHTMvh_AY8HS25SmZF64tndHDFE21gm)  
      
      - stopping parameter Θ = .001 (이 값이 작을수록 final value estimate이 더 정확해 질 것)  
     
      ![](http://drive.google.com/uc?export=view&id=19c8LqWpSt6jokydPYTNiwtpLX994qwVT)  
      ...  
      ![](http://drive.google.com/uc?export=view&id=1kRt4o8u4PlPU1CQa118xoD5l7O3N7q3o)  
      
#### Summary  
```  
- We can turn the Bellman equation into an update rule, to iteratively compute value functions.  
```  

## 4.2 Policy Iteration (Control)  
### 4.2.1 Policy Improvement  
- Recall  
  ![](http://drive.google.com/uc?export=view&id=19kyqhMdzTX_U4HelzBQzzCIgFoikhOmD)  
  
  ```  
  Previously we showed that given v*, we can find the optimal policy by choosing 
  the Greedy action. The Greedy action maximizes the Bellman's optimality equation
  in each state. 
  
  Imagine instead of the optimal value function, we select an action which is greedy
  with respect to the value function v_π of an arbitrary policy π in which case 
  π is already optimal.  
  What can we say about this new policy that is greedy with respect to v_π? 
  The first thing to note is that this new policy must be different than π.
  If this greedification doesn't change π, then π was already greedy with respect 
  to its own value function. This is just another way of saying that v_π obeys the 
  Bellmaan's optimality equation.  
  ```  

  - 이 방식으로 획득된 새로운 policy는 π에 대한 strict improvement여야 함.  
    (π가 이미 optimal 한 게 아니라면)  
  
  ![](http://drive.google.com/uc?export=view&id=14ArWMf8GxBpoqss9DfNZU3Nrs8hP5XKI)  
  
  ```  
  - Policy π' is at least as good as π if in each state, the value of an action selected
    by π' is greater than or equal to the value of the action selected by π. 
  - 〃 strictliy 〃  
  ```  
  
  ![](http://drive.google.com/uc?export=view&id=146q3dp-47p6Wa5EzWi4Fz54P21fM4p-W)  
  - Policy Improvement Theorm에 따르면, 새로운 policy는 처음에 시작할 때 정의했던  
    uniform random policy에 비해 improvement가 있었을 것이라는 게 보장되어 있음.  
    (`The new policy is guaranteed to be an improvement on the uniform random policy`  
     `we started with according to the policy improvement theorem`)  
     - In fact, if you look more closely at the new policy, we can see that it is in  
      fact optimal. In every state, the chosen actions lie on the shortest path to  
      the terminal state. Remember, the value function we started with was not the  
      optimal value function, and yet the greedy policy with respect to v_π is optimal.  
    
  - More generally, the policy improvement theorem only guarantees that the new policy is  
    an improvement on original. We cannot always expect to find the optimal policy so easily.  
    
  ![](http://drive.google.com/uc?export=view&id=1_2X-gVa6D1i9aM79lZMBqVo9eOxkS1f8)  
  ![](http://drive.google.com/uc?export=view&id=1FszZpbehaOtaoj1aF6v85pUZ6JTq3m7N)  
  
#### Summary  
```  
- The Policy Improvement Theorem tells us that a greedified policy is a strict improvement(
  unless the original policy was already optimal).
- Use the value function under a given policy, to produce a strictly better policy. 
```  
    
### 4.2.2 Policy Iteration  
- We learned that the value function computed for a given policy can be used to find a better policy.  
  (Policy Improvement)  
  
- Recall    
  - Policy Improvement Theorem  
  
    ```  
    Can construct a strictly better policy by acting greedily w.r.t the value function 
    of a given policy, unless the given p;olicy was already optimal.
    ```  
    
- Policy Iteration  

  ```  
  π'라고 하는 더 나은 policy를 만들기 위해 v_π를 사용해서 policy π를 개선(Improve)시키면,  
  v_π를 계산할 수 있고, 이를 또 반복해서 더 나은 policy인 π''를 만들 수 있다.  
  이 반복되는 과정을 Policy Iteration이라고 함.  
  ```  
  
  - Evaluation과 Improvement의 반복!!!  
  ![](http://drive.google.com/uc?export=view&id=1iEV7xjZGX_-nimdgFR2SXxERr8CPbscc)  
  - 점점 더 나은 policy가 되어 간다.  
  - 마지막 policy가 이미 optimal policy가 아니라면, 계속해서 improve될 것이라는 건 보장!  
  - Iteration이 끝나고 policy가 더 이상 바뀌지 않는다면 optimal policy를 찾은 것!  
  - 찾으면 algorithm terminate!  
  - (단, 여기서 모든 policy들은 다 deterministic policy)  
  
    ```  
    Finite MDP는 단지 유한한 policy를 갖고 있기 때문에 Evaluation과 Imporvement를 반복하는 과정은  
    유한 번 이내에 optimal policy와 optimal value function으로 반드시 수렴하게 된다.
    ```  
    
  ![](https://dnddnjs.gitbooks.io/rl/content/6d484ed095cba2cd7a8edf50b7e4e17e.png)  

  - This dance of policy and value proceeds back and forth, until we reach the only  
    policy, which is greedy with respect to its own value function, the optimal policy.  
    At this point, and only at this point, the policy is greedy and the value function is  
    accurate.  
  - We can visualize this dance as bouncing back and forth between one one, where the  
    value function is accurate, and another where the policy is greedy. These two lines  
    intersect only at the optimal policy and value function.  
    
  ![](http://drive.google.com/uc?export=view&id=1b7fgkyeBAoYSPVls3N2fnc136NKojFLE)  
  ![](http://drive.google.com/uc?export=view&id=19s7CYZ8d0jFEKd-dHI6XLw5GKUIm8jHp)  
  └ 계속 Evaluate, Improve하다보니 다음과 같은 path 완성!: Policy iteration의 힘!  
  
- Policy iteration은 optimal policy 찾기가 그리 간단하지 않을 때, search space를 줄여줌  

#### Summary  

```  
- Policy Iteration works by alternating policy evaluation and policy improvment.  
- Policy iteration follows a sequence of better and better policies and value functions 
  until it reduces the optimal policy and associated optimal value function.  
```  
    
  
## 4.3 Generalized Policy Iteration  
## 4.3.1 Flexibility of the Policy Iteration Framework   

- Policy Iteration (from above)  
  - alternate between `evaluating` the current policy and greedify to `improve` the policy.  
  - 꽤나 rigid한 procedure 같아 보임  
  
- 반면, `Generalized Policy Iteration`은 less rigid (much more freedom)  
  - allows `much more freedom` while `maintaining our optimality guarantees`  

- Generalized Policy Iteration  
![](http://drive.google.com/uc?export=view&id=1nhyOlDDvRsbxMaeYqcyqoEj8rGWgXVbA)  
(이 전 그림과 비교)  
  (교재 표현: `It may be possible to truncate policy evaluation.`)  
- Each policy improvement step makes our policy a little more greedy, but `not totally greedy`.  
  - 하지만, 여전히 optimal policy와 optimal value function을 향해 갈 수 있어야.  
- GPI → Evaluation과 Policy Improvement를 교차배치(Interleave)할 수 있는 모든 방법들을 가리킴  

  - GPI Algorithm 1: `Value Iteration`     
    - Still sweep over all the states and greedify with respect to the current value function.  
    - `However`, do not run policy evaluation to completion.  
      - `Just one sweep (one update of each state)` over all the states.  
      - 그 후, greedify again. 
    ![](http://drive.google.com/uc?export=view&id=1CGgZv_2rExI2MS7_1r_TzUs_qIz1dsAU)  
    - Iterative policy evaluation과 매우 유사   
    - 다만 고정된 policy에 따라 업데이트하지 않고, current value estimate을 최대화하는 action을 사용하여 업데이트.  
    - 극한의 관점에서 value iteration은 여전히 ![](https://latex.codecogs.com/gif.latex?v_*)으로 수렴  
    
- Value iteration은 (policy iteration처럼) 매 iteration마다 모든 state space를 sweep  
  - `Synchronous` dynamic programming algo    
    - 굉장히 systematic하게 sweep (한 칸, 그 옆의 한 칸, 또 그 옆의 한 칸...)  
    - state space가 커질 경우 문제.  
      - 매 sweep이 엄청난 시간 소요 가능)  
  - `Asynchronous` dynamic programming algo  
    - 순서 뒤죽박죽 (arbitrary order) sweep  
      - 어떤 state는 다른 어떤 state가 한 번도 update되지 못했음에도, 수 차례 update 될 수도 있음  
      - 수렴 보장을 위해서는, 모든 state 값들을 업데이트 하기는 해야만 돼  
      - selective update! (빠른 value information propagation 가능)  
      - Synchronous보다 훨씬 효율적일 수 있다. (ex. 최근에 update된 value 부근에서 update)  
      
#### Summary  

```  
- Value Iteration allows us to combine policy evaluation and improvement into a single update  
- Asynchronous dynamic programming methods give us the freedom to update states in any order 
- Generalized Policy Iteration unifies classical DP methods, value iteration, and asynchronous DP
```  

### 4.3.2 Efficiency of Dynamic Programming  
- Dynamic Programming Method가 value function과 policy를 계산할 수 있게 한다는 것을 배움.  
- 근데 과연 이 방법들이 얼마나 유용할까?  
  - 이 강의에서, 다른 가능한 솔루션들과 비교하여 Dynamic Programming이 얼마나 효율적인 방법인지를 깨달을 수 있으 것  

- Iterative policy evaluation  
  : Dynamic Programming solution to the `prediction` and `policy iteration problem`.  
  
  cf) `Sample-based alternative` for policy evaluation  
  
- Sample-based Alternative for Policy Evaluation  
  - The value of each state can be treated as a totally independent estimation problem.  
  
  - (Recall) Value:  
    ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D%28s%29%20%5Cdoteq%20E_%7B%5Cpi%7D%5BG_t%7CS_t%3Ds%5D)  
  - π 아래에 많은 수의 Return 값들을 모으고 avg → 결국엔 state value로 수렴할 것: `Monte Carlo Method`   
    - 하지만, 이 방법은 각 state마다 엄청난 수의 return을 필요로 함.  

    
  - 이에 반해 Dynamic Programming은 각 state의 evaluation을 독립된 문제로 보지 않아도 됨.  
    - 다른 value estimate을 사용해도 됨  
    - `Bootstrapping`  
      : 현재 value estimate을 improve 하기 위해 다음 state의 value estimate을 사용하는 과정  
      - Bootstrapping은 (각 value들을 독립적으로 추정하는) Monte Carlo보다 훨씬 효율적  
    
- Brute-force search  
  - optimal policy를 계산함에 있어서 Policy Iteration의 가능 대안  
  - 단순히 모든 가능한 deterministic policy를 차례로 evaluate하고, 가장 큰 값을 가진 애를 선택  
  - 유한한 deterministic policy → 항상 optimal deterministic policy 존재  
  - 따라서 결국에는 답을 찾을 것. 하지만 deterministic policy의 수가 거대할 수도 있다.  
  
  - Deterministic policy: one action choice per state  
    - Deterministic policy의 총 개수: `exponential` in the number of states  
    - 정말 단순한 문제에서도 이 수는 굉장히 커질 수 있다. → 시간 오래 걸림  
    
  - cf) Policy Iteration: state와 action 수에 대해 polynomial 시간 동안 optimal policy 발견 보장.  
  - Dynamic Programming: brute-force search보다 exponentially faster  
  
- 일반적으로, MDP의 해결은 state 수가 많아짐에 따라 어려워짐.  
  - 차원의 저주 → state 변수가 하나 늘어날 때, state space는 exponential 하게 늘어난다.  
    - 하나의 agent가 Grid를 돌아다니는 건 ㄱㅊ  
    - 근데, 수 백개의 location을 수천명의 운전사가 돌아다닌다면..?  
      - A raw enumeration of the possible states could lead to an exponential blow-up  
      - This would lead to problems if we try to sweep the states to perform policy iteration.  
      → Dynamic Programming 에서는 문제가 되지 않는다.  
      
        ```  
        The size of the state space grows exponentially as the number of relevant features increases.  
        
        This is not an issue with Dynamic Programming, but an inherent complexity of the problem.  
        ```  
        
```  

교재:  
We use th term GPI to refer to the general idea of letting policy-evaluation and policy-improvement  
processess interact, independent of the granularity oand other details of th two procesess. Almost  
all RL methods are well described as GPI. That is, all have identifiable policies and value functions, 
with the policy always being improed with respect to the value funciton and the value function always 
being driven toward the value function for the policy. 
...  
The value function stabilizes only when it is consistent with the current policy, and the policy 
stabilizes only when it is greedy with respect to the current value function. 
Thus, both processes stabilize only when a poicy has been found that is greedy with respect to 
its own evaluation function. 
```
      
#### Summary  

```  
- Bootstrapping can save us from performing a huge amount of unnecessary work.  
  (by exploiting the connection between the value of a state and its possible successors.  
```  
    
    
### 4.3.5 Week 4 Summary  
- Dynamic Programming can be used to solve the tasks of policy evaluation and control.  
  - RL algo's foundation   

- `Policy Evaluation`  
  : 특정 policy π에 대한 state-value function ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D) 를 결정하는 task.  
  
  - `Iterative Policy Evaluation`  
    : Takes the Bellman equation for !][](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D) and turns it into an `update` rule.  
    ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D%28s%29%20%3D%20%5Csum_a%5Cpi%28a%7Cs%29%5Csum_%7Bs%27%7D%5Csum_r%20p%28s%27%2Cr%7Cs%2Ca%29%5Br&plus;%5Cgamma%20v_%7B%5Cpi%7D%28s%27%29%5D)  
    ↓  
    ![](https://latex.codecogs.com/gif.latex?v_%7Bk&plus;1%7D%28s%29%20%5Cleftarrow%20%5Csum_a%5Cpi%28a%7Cs%29%5Csum_%7Bs%27%7D%5Csum_r%20p%28s%27%2Cr%7Cs%2Ca%29%5Br&plus;%5Cgamma%20v_%7Bk%7D%28s%27%29%5D)  
    - Produces a sequene of better and better approximations to ![](https://latex.codecogs.com/gif.latex?v_%7B%5Cpi%7D)  
    
- `Control`  
  : Policy를 향상시키는 task (task of improving a policy)  
  : how to construct a better policy from a given policy  
  - `Policy Improvement Theorem`  
    ![](https://latex.codecogs.com/gif.latex?%5Cpi%27%28s%29%20%5Cdoteq%20%5Cunderset%7Ba%7D%7Bargmax%7D%5Csum_%7Bs%27%7D%5Csum_r%20p%28s%27r%7Cs%2Ca%29%5Br&plus;%5Cgamma%20v_%7B%5Cpi%7D%28s%27%29%5D)  
    π' > π unless π is optimal  
  - Dynamic programming algorithm for control is built on the policy improvement theorem.  
    : Is called `Policy Iteration`  
      - Policy Iteration의 2 step  
      
        ```  
        1. Policy evaluation  
          - iterative computation of the value functions for a given policy
        2. Policy Improvement
          - computation of an improved policy given the value funciton for that policy  
        ```  
        
        - 1, 2 의 반복(을 통해 `policy iteration`과 `value iteration`을 획득)  

  - Generalized Policy Iteration  
    - Evaluation & Improvement steps need not run to completion.  
      (: General idea of two interacting processes revolving around an `approximate` policy  
        and an `approximate` value function`)  
    
    - Generalized Policy Iteration includes `asynchronous` dynamic programming methods.  
      - Asynchronous: can be especially helful when the state space is very large.  
      - Asynchronous: can be designed to focus on a few relevant states.  
      
- In dynamic programming, in some sense assume the best possible situation.  

---  

### Reference  
- Coursera
